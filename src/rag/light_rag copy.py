"""
âœ… FIXED: LightRAG - RAG à¸—à¸µà¹ˆà¹ƒà¸Šà¹‰ Knowledge Graph à¸ªà¸³à¸«à¸£à¸±à¸š FastAPI

ðŸ”§ FIX SUMMARY:
1. âœ… Event Loop Management - à¸ˆà¸±à¸”à¸à¸²à¸£ async properly
2. âœ… nest_asyncio replacement - à¹ƒà¸Šà¹‰ Threading à¹à¸—à¸™
3. âœ… Close loop after use - à¸«à¸¥à¸µà¸à¹€à¸¥à¸µà¹ˆà¸¢à¸‡ "Event loop is closed"
4. âœ… Separate thread for queries - à¹„à¸¡à¹ˆ block FastAPI event loop
5. âœ… Proper async/await patterns

ðŸ“Š Knowledge Graph Architecture:
- Entity Extraction: à¸”à¸¶à¸‡ "à¸ªà¸´à¹ˆà¸‡" (à¸„à¸™, à¸ªà¸–à¸²à¸™à¸—à¸µà¹ˆ, à¸ªà¸´à¸™à¸„à¹‰à¸²) à¸ˆà¸²à¸à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡
- Relation Extraction: à¸”à¸¶à¸‡ "à¸„à¸§à¸²à¸¡à¸ªà¸±à¸¡à¸žà¸±à¸™à¸˜à¹Œ" à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡ entities
- Graph Storage: à¹€à¸à¹‡à¸šà¹ƒà¸™ GraphML format (lightrag_db/graph_chunk_entity_relation.graphml)
- Multi-hop Reasoning: à¸•à¸²à¸¡à¹€à¸™à¸·à¹‰à¸­à¸‡à¸²à¸™ entities à¸œà¹ˆà¸²à¸™ relationships

ðŸŽ¯ 4 Query Modes:
1. naive: Vector search à¹à¸šà¸šà¸˜à¸£à¸£à¸¡à¸”à¸² (à¹€à¸£à¹‡à¸§à¸ªà¸¸à¸” à¹à¸•à¹ˆà¸œà¸¥à¸™à¹‰à¸­à¸¢)
2. local: à¸„à¹‰à¸™à¸«à¸² entities à¸—à¸µà¹ˆà¹€à¸à¸µà¹ˆà¸¢à¸§à¸‚à¹‰à¸­à¸‡ + neighbors
3. global: à¸”à¸¶à¸‡ global summary à¸‚à¸­à¸‡ graph (à¹€à¸«à¸¡à¸²à¸° overview)
4. hybrid: local + global (à¸”à¸µà¸—à¸µà¹ˆà¸ªà¸¸à¸” - à¹à¸™à¸°à¸™à¸³à¹ƒà¸Šà¹‰à¸™à¸µà¹ˆ)
"""

import os
import asyncio
import threading
import logging
from pathlib import Path
from typing import List, Dict, Optional, Any
from concurrent.futures import ThreadPoolExecutor
import queue
import time

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Import base RAG
try:
    from .base_rag import BaseRAG
except ImportError:
    from base_rag import BaseRAG

# Try to import LightRAG
try:
    from lightrag import LightRAG, QueryParam
    from lightrag.kg.shared_storage import initialize_pipeline_status
    HAS_LIGHTRAG = True
    logger.info("âœ… LightRAG imported successfully")
except ImportError as e:
    HAS_LIGHTRAG = False
    logger.warning(f"âš ï¸ LightRAG not available: {e}")
    logger.info("ðŸ’¡ Install with: pip install lightrag-hku networkx nano-vectordb")


class AsyncQueryExecutor:
    """
    ðŸ”§ Async Query Executor - à¸ˆà¸±à¸”à¸à¸²à¸£ async operations à¹ƒà¸™ sync context
    
    à¸›à¸±à¸à¸«à¸²: FastAPI à¸¡à¸µ event loop à¸—à¸µà¹ˆà¸£à¸±à¸™à¸­à¸¢à¸¹à¹ˆà¹à¸¥à¹‰à¸§
    - loop.run_until_complete() à¸ˆà¸° error à¹€à¸žà¸£à¸²à¸° loop already running
    - nest_asyncio.apply() à¹„à¸¡à¹ˆà¸¡à¸±à¹ˆà¸™à¹ƒà¸ˆ
    
    à¸§à¸´à¸˜à¸µà¹à¸à¹‰: à¹ƒà¸Šà¹‰ Thread Pool à¹€à¸žà¸·à¹ˆà¸­ isolate async operations
    """
    
    def __init__(self, max_workers: int = 2):
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.logger = logger
    
    async def run_query(self, query_func, query_text: str, timeout: int = 30) -> Optional[str]:
        """
        à¸£à¸±à¸™ async query à¹ƒà¸™ thread à¸—à¸µà¹ˆà¸›à¸¥à¸­à¸”à¸ à¸±à¸¢
        
        ðŸ“ à¸§à¸´à¸˜à¸µà¸à¸²à¸£à¸—à¸³à¸‡à¸²à¸™:
        1. à¸ªà¸£à¹‰à¸²à¸‡ thread à¹ƒà¸«à¸¡à¹ˆà¸”à¹‰à¸§à¸¢ ThreadPoolExecutor
        2. à¹ƒà¸™ thread à¸™à¸±à¹‰à¸™ à¸ªà¸£à¹‰à¸²à¸‡ event loop à¹ƒà¸«à¸¡à¹ˆ
        3. à¸£à¸±à¸™ query function à¹ƒà¸™ loop à¸™à¸±à¹‰à¸™
        4. à¸›à¸´à¸” loop à¸­à¸¢à¹ˆà¸²à¸‡à¸ªà¸¡à¹ˆà¸³à¹€à¸ªà¸¡à¸­ (à¸ªà¸³à¸„à¸±à¸à¸¡à¸²à¸!)
        """
        def _run_query_in_thread():
            try:
                # à¸ªà¸£à¹‰à¸²à¸‡ event loop à¹ƒà¸«à¸¡à¹ˆà¹ƒà¸™à¸•à¸±à¸§ thread à¸™à¸µà¹‰ (à¹„à¸¡à¹ˆ share à¸à¸±à¸š FastAPI)
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                
                try:
                    # à¸£à¸±à¸™ async query
                    result = loop.run_until_complete(
                        asyncio.wait_for(query_func(query_text), timeout=timeout)
                    )
                    return result
                finally:
                    # ðŸ”´ CRITICAL: à¸›à¸´à¸” loop à¸­à¸¢à¹ˆà¸²à¸‡à¸–à¸¹à¸à¸•à¹‰à¸­à¸‡
                    pending = asyncio.all_tasks(loop)
                    for task in pending:
                        task.cancel()
                    loop.run_until_complete(asyncio.gather(*pending, return_exceptions=True))
                    loop.close()
                    self.logger.debug("âœ… Event loop closed properly in thread")
                    
            except asyncio.TimeoutError:
                self.logger.error(f"â±ï¸ Query timeout after {timeout}s")
                raise
            except Exception as e:
                self.logger.error(f"âŒ Query execution error: {e}")
                raise
        
        # à¸£à¸±à¸™ thread
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(self.executor, _run_query_in_thread)
    
    def shutdown(self):
        """à¸›à¸´à¸” executor"""
        self.executor.shutdown(wait=True)
        self.logger.info("âœ… AsyncQueryExecutor shutdown")


class LightRAGWrapper(BaseRAG):
    """
    âœ… FIXED LightRAG Wrapper - Knowledge Graph RAG for FastAPI
    
    ðŸ§  à¸§à¸´à¸˜à¸µà¸à¸²à¸£à¸—à¸³à¸‡à¸²à¸™:
    
    1ï¸âƒ£ DOCUMENT PROCESSING (à¸ªà¸£à¹‰à¸²à¸‡ Knowledge Graph)
       - à¹€à¸­à¸à¸ªà¸²à¸£à¹€à¸‚à¹‰à¸² â†’ Chunking â†’ Entity Extraction (LLM)
       - Entity: "à¸£à¹‰à¸²à¸™ Krua Baan Thai", "à¸‚à¹‰à¸²à¸§à¸œà¸±à¸”à¸à¸¸à¹‰à¸‡", "à¸£à¸²à¸„à¸² 180 à¸šà¸²à¸—"
       - Relations: "Krua Baan Thai --serves--> à¸‚à¹‰à¸²à¸§à¸œà¸±à¸”à¸à¸¸à¹‰à¸‡"
       â†’ Graph Structure: nodes=entities, edges=relations
    
    2ï¸âƒ£ GRAPH CONSTRUCTION
       - Merge: entities à¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸™ à¸–à¹‰à¸²à¸«à¸¡à¸²à¸¢à¸–à¸¶à¸‡à¹€à¸£à¸·à¹ˆà¸­à¸‡à¹€à¸”à¸µà¸¢à¸§
       - Storage: à¹ƒà¸Šà¹‰ nano-vectordb (vector) + GraphML (structure)
    
    3ï¸âƒ£ QUERY TIME - Multi-hop Reasoning
       Query: "à¸£à¹‰à¸²à¸™à¹„à¸«à¸™à¸¡à¸µà¸­à¸°à¹„à¸£à¸­à¸£à¹ˆà¸­à¸¢à¸–à¸¹à¸à¹†?"
       
       a) Local Mode: à¸„à¹‰à¸™à¸«à¸² "à¸£à¹‰à¸²à¸™" â†’ à¹€à¸žà¸·à¹ˆà¸­à¸™à¸šà¹‰à¸²à¸™à¸‚à¸­à¸‡à¸¡à¸±à¸™ (neighbors)
          "à¸£à¹‰à¸²à¸™" --serves--> "à¹€à¸¡à¸™à¸¹" --has_price--> "à¸šà¸²à¸—"
       
       b) Global Mode: à¸”à¸¶à¸‡ "à¸ªà¸£à¸¸à¸›" à¸—à¸±à¹‰à¸‡ graph
          "In this document, there are 5 restaurants..."
       
       c) Hybrid (Best): local + global combined
    
    4ï¸âƒ£ RESPONSE GENERATION
       Context: "Krua Baan Thai serves Pad Gung for 180 baht"
       LLM: "à¸£à¹‰à¸²à¸™à¹€à¸§à¸´à¹‰à¸‡à¸šà¹‰à¸²à¸™à¹„à¸—à¸¢ à¸¡à¸µà¸‚à¹‰à¸²à¸§à¸œà¸±à¸”à¸à¸¸à¹‰à¸‡ à¸£à¸²à¸„à¸² 180 à¸šà¸²à¸— à¸–à¸¹à¸à¸¡à¸²à¸à¸„à¸£à¸±à¸š!"
    """
    
    def __init__(self, llm_handler, memory_handler, query_mode: str = "hybrid"):
        """
        Initialize LightRAG Wrapper
        
        Args:
            llm_handler: LLM Handler instance
            memory_handler: Memory Handler instance
            query_mode: "naive", "local", "global", or "hybrid"
        """
        super().__init__(llm_handler, memory_handler, name="LightRAG")
        
        if not HAS_LIGHTRAG:
            raise ImportError(
                "LightRAG not installed. Install with:\n"
                "pip install lightrag-hku networkx nano-vectordb"
            )
        
        self.query_mode = query_mode
        self.working_dir = "./data/lightrag_db"
        os.makedirs(self.working_dir, exist_ok=True)
        
        # Initialize LightRAG instance
        self.light_rag = None
        self._initialize_lightrag()
        
        # âœ… NEW: Async query executor for FastAPI compatibility
        self.query_executor = AsyncQueryExecutor(max_workers=2)
        
        logger.info(f"âœ… LightRAG Ready at {self.working_dir}")
        logger.info(f"ðŸ” Query Mode: {self.query_mode}")
        logger.info(f"ðŸ§µ Async Executor: Ready with 2 threads")
    
    def _initialize_lightrag(self):
        """à¹€à¸•à¸£à¸µà¸¢à¸¡ LightRAG à¸”à¹‰à¸§à¸¢ Ollama backend"""
        
        # âœ… LLM Wrapper - Callable class
        class OllamaLLMWrapper:
            """Wrapper for Ollama LLM that LightRAG can call"""
            def __init__(self, llm_handler):
                self.llm_handler = llm_handler
                self.func = self.__call__
            
            async def __call__(
                self,
                prompt: str,
                system_prompt: Optional[str] = None,
                history_messages: List = [],
                **kwargs
            ) -> str:
                """
                Async LLM generation (called by LightRAG)
                
                ðŸ“ LightRAG expects async callable
                """
                try:
                    full_prompt = prompt
                    if system_prompt:
                        full_prompt = f"{system_prompt}\n\n{prompt}"
                    
                    # âœ… Run sync LLM in executor
                    loop = asyncio.get_event_loop()
                    response = await loop.run_in_executor(
                        None,
                        self.llm_handler.generate,
                        full_prompt
                    )
                    return response
                    
                except Exception as e:
                    logger.error(f"LLM generation error: {e}")
                    return f"Error: {str(e)}"
        
        ollama_wrapper = OllamaLLMWrapper(self.llm)
        
        # âœ… Embedding Wrapper with proper async
        class EmbeddingWrapper:
            """Wrapper for embeddings (with embedding_dim attribute required by LightRAG)"""
            
            def __init__(self, embedding_model):
                self.embedding_model = embedding_model
                
                # ðŸ”‘ CRITICAL: Get embedding_dim (LightRAG requires this!)
                try:
                    if hasattr(embedding_model, 'get_sentence_embedding_dimension'):
                        self.embedding_dim = embedding_model.get_sentence_embedding_dimension()
                        logger.info(f"âœ… Embedding dim (method 1): {self.embedding_dim}")
                    elif hasattr(embedding_model, 'config') and hasattr(embedding_model.config, 'hidden_size'):
                        self.embedding_dim = embedding_model.config.hidden_size
                        logger.info(f"âœ… Embedding dim (method 2): {self.embedding_dim}")
                    else:
                        test_embed = embedding_model.encode(["test"], convert_to_numpy=True)
                        self.embedding_dim = test_embed.shape[1]
                        logger.info(f"âœ… Embedding dim (method 3): {self.embedding_dim}")
                except Exception as e:
                    logger.warning(f"âš ï¸ Auto-detect failed: {e}")
                    self.embedding_dim = 384
                
                logger.info(f"ðŸ“Š Final embedding_dim: {self.embedding_dim}")
            
            async def __call__(self, texts: List[str]):
                """
                Generate embeddings (MUST be async for LightRAG)
                
                ðŸ“ à¸§à¸´à¸˜à¸µ:
                1. Convert string â†’ list
                2. Run in executor (à¹„à¸¡à¹ˆ block event loop)
                3. Return numpy array (len(texts), embedding_dim)
                """
                try:
                    import numpy as np
                    
                    if isinstance(texts, str):
                        texts = [texts]
                    
                    if not texts:
                        return np.zeros((0, self.embedding_dim))
                    
                    loop = asyncio.get_event_loop()
                    embeddings = await loop.run_in_executor(
                        None,
                        lambda: self.embedding_model.encode(
                            texts,
                            normalize_embeddings=True,
                            convert_to_numpy=True,
                            show_progress_bar=False
                        )
                    )
                    
                    return embeddings
                    
                except Exception as e:
                    logger.error(f"Embedding error: {e}")
                    import numpy as np
                    return np.zeros((len(texts) if isinstance(texts, list) else 1, self.embedding_dim))
        
        # Get embedding model from memory
        if hasattr(self.memory, 'model'):
            embedding_model = self.memory.model
        elif hasattr(self.memory, 'embedding_model'):
            embedding_model = self.memory.embedding_model
        else:
            raise AttributeError("Memory has no 'model' or 'embedding_model' attribute")
        
        embedding_func = EmbeddingWrapper(embedding_model)
        
        try:
            logger.info("ðŸ”§ Initializing LightRAG...")
            self.light_rag = LightRAG(
                working_dir=self.working_dir,
                llm_model_func=ollama_wrapper,
                embedding_func=embedding_func,
            )
            
            # Initialize storages
            self._init_storages_sync()
            logger.info("âœ… LightRAG initialized successfully")
            
        except Exception as e:
            logger.error(f"LightRAG initialization failed: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise RuntimeError(f"LightRAG init failed: {e}")
    
    def _init_storages_sync(self):
        """Initialize storages in a clean thread (not in FastAPI event loop)"""
        def _init_in_thread():
            try:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                
                async def init():
                    try:
                        await self.light_rag.initialize_storages()
                        logger.info("âœ… Initialized storages")
                    except Exception as e:
                        logger.debug(f"Storage init note: {e}")
                    
                    try:
                        await initialize_pipeline_status()
                        logger.info("âœ… Initialized pipeline status")
                    except Exception as e:
                        logger.debug(f"Pipeline init note: {e}")
                
                loop.run_until_complete(init())
                loop.close()
            except Exception as e:
                logger.warning(f"Storage init warning: {e}")
        
        # Run in thread
        init_thread = threading.Thread(target=_init_in_thread, daemon=False)
        init_thread.start()
        init_thread.join(timeout=10)
    
    def retrieve(self, query: str, k: int = 3) -> List[Dict]:
        """
        à¸„à¹‰à¸™à¸«à¸²à¸”à¹‰à¸§à¸¢ LightRAG graph search
        
        ðŸŽ¯ Process:
        1. Check if database has data
        2. Create QueryParam with current mode
        3. Execute async query (safely in thread)
        4. Format result for BaseRAG interface
        
        Args:
            query: Question
            k: Number of results (not used by LightRAG)
            
        Returns:
            List of results in standard format
        """
        try:
            # Check initialization
            if not self.light_rag:
                logger.error("LightRAG not initialized")
                return self._error_result("LightRAG not initialized")
            
            # Check if database has data
            graph_file = Path(self.working_dir) / "graph_chunk_entity_relation.graphml"
            if not graph_file.exists():
                logger.warning("âš ï¸ Knowledge Graph database is empty!")
                return self._error_result(
                    "No data in Knowledge Graph. Please upload documents first.",
                    "empty_database"
                )
            
            logger.info(f"ðŸ” Querying LightRAG (mode: {self.query_mode}): {query[:50]}...")
            
            # Create query param
            query_param = QueryParam(mode=self.query_mode)
            
            # âœ… NEW: Use AsyncQueryExecutor for safe async execution
            # âœ… FIXED: Use ThreadPoolExecutor instead of asyncio.run()
            try:
                import concurrent.futures
                
                def run_query_in_thread():
                    """Run async query in separate thread with new event loop"""
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    try:
                        result = loop.run_until_complete(
                            self.light_rag.aquery(query, param=query_param)
                        )
                        return result
                    finally:
                        # Clean up properly
                        pending = asyncio.all_tasks(loop)
                        for task in pending:
                            task.cancel()
                        loop.run_until_complete(asyncio.gather(*pending, return_exceptions=True))
                        loop.close()
                
                # Execute in thread pool with timeout
                with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                    future = executor.submit(run_query_in_thread)
                    result = future.result(timeout=30)
                    
            except concurrent.futures.TimeoutError:
                logger.error("Query timeout (30s)")
                return self._error_result("Query timeout. Please try again.", "timeout")
            except Exception as e:
                logger.error(f"Query error: {e}")
                import traceback
                logger.error(traceback.format_exc())
                return self._error_result(f"Query error: {str(e)}", "query_error")
            logger.info(f"âœ… Query complete: {len(result) if result else 0} chars")
            
            # Format result
            return [{
                'content': result if result else "No relevant information found",
                'score': 1.0,
                'metadata': {
                    'source': 'lightrag_graph',
                    'mode': self.query_mode,
                    'type': 'graph_search'
                }
            }]
            
        except Exception as e:
            logger.error(f"Retrieve error: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return self._error_result(str(e), "retrieve_error")
    
    def _error_result(self, message: str, error_type: str = "error") -> List[Dict]:
        """à¸ªà¸£à¹‰à¸²à¸‡ error result object"""
        return [{
            'content': message,
            'score': 0.0,
            'metadata': {'source': 'lightrag_error', 'type': error_type}
        }]
    
    def generate(self, query: str, context: str) -> str:
        """
        à¸ªà¸£à¹‰à¸²à¸‡à¸„à¸³à¸•à¸­à¸šà¸ˆà¸²à¸ context
        
        Note: LightRAG already generates answer in retrieve()
        so we just format it here
        """
        try:
            if context and context.strip():
                answer = context
                # Remove numbering like [1], [2] if present
                if "[1]" in answer:
                    lines = answer.split("\n")
                    clean_lines = [line for line in lines 
                                   if line.strip() and not (line.strip().startswith("[") and "]" in line[:5])]
                    answer = "\n".join(clean_lines)
                
                return answer.strip()
            
            # Fallback: use LLM directly
            logger.warning("âš ï¸ Empty context from LightRAG, using LLM fallback")
            prompt = f"""Question: {query}

Answer concisely in Thai:"""
            
            return self.llm.generate(prompt)
            
        except Exception as e:
            logger.error(f"Generation error: {e}")
            return f"Error generating answer: {str(e)}"
    
    def insert_documents(self, texts: List[str], metadatas: Optional[List[dict]] = None):
        """
        Insert documents into LightRAG Knowledge Graph
        
        ðŸ§  Process:
        1. Combine texts
        2. Run async insert in separate thread (important for FastAPI!)
        3. LightRAG extracts entities and relations automatically
        
        Args:
            texts: List of text chunks
            metadatas: Optional metadata (not used by LightRAG)
        """
        if not texts:
            logger.warning("No texts to insert")
            return
        
        if not self.light_rag:
            logger.error("LightRAG not initialized")
            return
        
        try:
            logger.info(f"ðŸ“š Inserting {len(texts)} documents into LightRAG...")
            combined_text = "\n\n".join(texts)
            
            # Run in background thread to not block FastAPI
            def insert_in_thread():
                try:
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    
                    async def do_insert():
                        # Re-initialize storages
                        try:
                            await self.light_rag.initialize_storages()
                            await initialize_pipeline_status()
                            logger.debug("âœ… Re-initialized storages in thread")
                        except Exception as e:
                            logger.debug(f"Storage re-init note: {e}")
                        
                        # Insert document
                        await self.light_rag.ainsert(combined_text)
                    
                    loop.run_until_complete(do_insert())
                    loop.close()
                    logger.info("âœ… Documents inserted into Knowledge Graph!")
                    
                except Exception as e:
                    logger.error(f"Insertion error: {e}")
                    import traceback
                    logger.error(traceback.format_exc())
            
            insert_thread = threading.Thread(target=insert_in_thread, daemon=False)
            insert_thread.start()
            logger.info("â„¹ï¸ Document insertion started in background...")
            
        except Exception as e:
            logger.error(f"Insert error: {e}")
            import traceback
            logger.error(traceback.format_exc())
    
    def set_query_mode(self, mode: str):
        """Change query mode"""
        valid_modes = ["naive", "local", "global", "hybrid"]
        if mode not in valid_modes:
            raise ValueError(f"Invalid mode. Must be one of: {valid_modes}")
        
        self.query_mode = mode
        logger.info(f"ðŸ”„ Query mode changed to: {mode}")
    
    def get_query_mode(self) -> str:
        """Get current query mode"""
        return self.query_mode
    
    def get_graph_stats(self) -> Dict:
        """Get knowledge graph statistics"""
        try:
            stats = {
                "working_dir": self.working_dir,
                "query_mode": self.query_mode,
                "status": "initialized"
            }
            
            graph_dir = Path(self.working_dir)
            if graph_dir.exists():
                files = list(graph_dir.glob("*"))
                stats["files_count"] = len(files)
                stats["has_graph"] = (graph_dir / "graph_chunk_entity_relation.graphml").exists()
                stats["storage_exists"] = True
                
                # Try to get entity count
                try:
                    import networkx as nx
                    graph_file = graph_dir / "graph_chunk_entity_relation.graphml"
                    if graph_file.exists():
                        G = nx.read_graphml(str(graph_file))
                        stats["entities"] = G.number_of_nodes()
                        stats["relations"] = G.number_of_edges()
                except Exception as e:
                    logger.debug(f"Could not read graph stats: {e}")
            else:
                stats["storage_exists"] = False
            
            return stats
            
        except Exception as e:
            logger.error(f"Stats error: {e}")
            return {"error": str(e)}
    
    def shutdown(self):
        """à¸›à¸´à¸” resources"""
        self.query_executor.shutdown()
        logger.info("âœ… LightRAGWrapper shutdown complete")


def is_lightrag_available() -> bool:
    """Check if LightRAG is available"""
    return HAS_LIGHTRAG