"""UI Components"""
import streamlit as st
from config import APP_TITLE, APP_DESCRIPTION, LLM_MODEL

def setup_page():
    st.set_page_config(
        page_title=APP_TITLE,
        page_icon="üí¨",
        layout="wide"
    )

def show_header():
    st.title(APP_TITLE)
    st.markdown(APP_DESCRIPTION)
    st.markdown("---")

def show_sidebar(memory_handler):
    """Show sidebar with document management"""
    with st.sidebar:
        st.header("‚öôÔ∏è Settings")
        
        # Model info
        st.subheader("ü§ñ Model")
        st.info(f"Using: {LLM_MODEL}")
        
        # Document stats
        st.subheader("üìö Knowledge Base")
        doc_count = memory_handler.count_documents()
        st.metric("Total Chunks", doc_count)
        
        # Show document list
        sources = memory_handler.get_all_sources()
        docs_to_delete = []  # Initialize here
        
        if sources:
            st.write(f"**Documents ({len(sources)}):**")
            
            # Create expander for document list
            with st.expander("üìÑ View Documents", expanded=False):
                for source in sources:
                    col1, col2 = st.columns([3, 1])
                    with col1:
                        st.text(f"üìÑ {source}")
                    with col2:
                        if st.button("üóëÔ∏è", key=f"delete_{source}", help="‡∏•‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ô‡∏µ‡πâ"):
                            docs_to_delete.append(source)
        else:
            st.info("‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£")
        
        # File uploader
        st.subheader("üì§ Upload Documents")
        uploaded_files = st.file_uploader(
            "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå",
            type=['txt', 'pdf', 'docx', 'md'],
            accept_multiple_files=True,
            help="‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö: TXT, PDF, DOCX, MD"
        )
        
        # Options
        st.subheader("üìù Options")
        clear_chat = st.button("üóëÔ∏è Clear Chat")
        clear_memory = st.button("üóëÔ∏è Clear All Documents", 
                                help="‡∏•‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î",
                                type="secondary")
        
        # About
        st.markdown("---")
        st.markdown("""
        **Personal AI Assistant**
        
        - ü§ñ Local LLM (Ollama)
        - üíæ Persistent Memory
        - üìö RAG Support
        - üîí 100% Private
        
        [GitHub](https://github.com/6amdev/personal-ai-assistant)
        """)
        
        # Return 4 values!
        return clear_chat, clear_memory, uploaded_files, docs_to_delete

def chat_interface(llm_handler, memory_handler):
    """Main chat interface with RAG"""
    
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # Display chat history
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # Chat input
    if prompt := st.chat_input("‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°..."):
        # Add user message
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Get AI response with RAG
        with st.chat_message("assistant"):
            with st.spinner("ü§î ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡∏¥‡∏î..."):
                # Get context from documents
                context = memory_handler.get_context(prompt, k=3)
                
                # Create full prompt
                if context:
                    full_prompt = f"""{context}

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {prompt}

‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏≠‡∏ö‡πÇ‡∏î‡∏¢‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Ç‡πâ‡∏≤‡∏á‡∏ï‡πâ‡∏ô (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ) ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ"""
                else:
                    full_prompt = prompt
                
                # Generate response
                response = llm_handler.generate(full_prompt)
                st.markdown(response)
                
                # Show context used
                if context:
                    with st.expander("üìö ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á"):
                        st.text(context)
        
        # Add to history
        st.session_state.messages.append({"role": "assistant", "content": response})

def clear_chat_history():
    """Clear chat history"""
    st.session_state.messages = []
    st.success("‚úÖ ‡∏•‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÅ‡∏•‡πâ‡∏ß!")