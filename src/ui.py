"""UI Components - With Model Selector & Multi-RAG Support (including LightRAG)"""
import streamlit as st
import subprocess
import json
import base64
from pathlib import Path
from config import APP_TITLE, APP_DESCRIPTION, LLM_MODEL
from src.utils import ImageHandler


def setup_page():
    st.set_page_config(
        page_title=APP_TITLE,
        page_icon="üí¨",
        layout="wide"
    )


def show_header():
    st.title(APP_TITLE)
    st.markdown(APP_DESCRIPTION)
    st.markdown("---")


def get_available_models() -> list:
    """
    ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ LLM models (‡∏Å‡∏£‡∏≠‡∏á Embedding models ‡∏≠‡∏≠‡∏Å)
    
    Returns:
        List ‡∏Ç‡∏≠‡∏á‡∏ä‡∏∑‡πà‡∏≠ LLM model
    """
    try:
        result = subprocess.run(
            ['ollama', 'list'],
            capture_output=True,
            text=True,
            timeout=5
        )
        
        if result.returncode == 0:
            lines = result.stdout.strip().split('\n')[1:]  # ‡∏Ç‡πâ‡∏≤‡∏° header
            models = []
            
            # Blacklist embedding models
            embedding_keywords = [
                'embed',
                'embedding',
                'nomic-embed',
                'bge-',
                'e5-',
                'gte-'
            ]
            
            for line in lines:
                if line.strip():
                    parts = line.split()
                    if parts:
                        model_name = parts[0]
                        
                        # ‡∏Å‡∏£‡∏≠‡∏á embedding models ‡∏≠‡∏≠‡∏Å
                        is_embedding = any(keyword in model_name.lower() for keyword in embedding_keywords)
                        
                        if not is_embedding:
                            models.append(model_name)
            
            return models if models else [LLM_MODEL]
        else:
            return [LLM_MODEL]
            
    except Exception as e:
        print(f"‚ö†Ô∏è Error getting models: {e}")
        return [LLM_MODEL]


def get_model_info(model_name: str) -> dict:
    """
    ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• model ‡∏à‡∏≤‡∏Å Ollama
    
    Args:
        model_name: ‡∏ä‡∏∑‡πà‡∏≠ model
        
    Returns:
        Dict ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• model
    """
    try:
        result = subprocess.run(
            ['ollama', 'show', model_name],
            capture_output=True,
            text=True,
            timeout=5
        )
        
        if result.returncode == 0:
            info = {}
            lines = result.stdout.split('\n')
            
            for line in lines:
                if ':' in line and not line.strip().startswith('#'):
                    parts = line.split(':', 1)
                    if len(parts) == 2:
                        key = parts[0].strip()
                        value = parts[1].strip()
                        if key and value:
                            info[key] = value
            
            return info if info else {"status": "Model info available"}
        else:
            return {"error": "Cannot get model info"}
            
    except Exception as e:
        return {"error": str(e)}


def show_sidebar(memory_handler):
    """Show sidebar with document management"""
    with st.sidebar:
        st.header("‚öôÔ∏è Settings")

        # üÜï Model Selector
        st.subheader("ü§ñ LLM Model")
        
        # ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ models
        available_models = get_available_models()
        
        # ‡πÄ‡∏Å‡πá‡∏ö current model ‡πÉ‡∏ô session state
        if 'selected_model' not in st.session_state:
            st.session_state.selected_model = LLM_MODEL
        
        current_model = st.session_state.selected_model
        
        # ‡∏´‡∏≤ index ‡∏Ç‡∏≠‡∏á current model
        try:
            current_index = available_models.index(current_model)
        except ValueError:
            current_index = 0
            st.session_state.selected_model = available_models[0] if available_models else LLM_MODEL
        
        selected_model = st.selectbox(
            "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Model",
            available_models,
            index=current_index,
            help="Model ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°"
        )
        
        # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô model ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if selected_model != st.session_state.selected_model:
            st.session_state.selected_model = selected_model
            st.session_state.model_changed = True
            st.success(f"‚úÖ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô {selected_model}")
            st.info("üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î model ‡πÉ‡∏´‡∏°‡πà...")
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• model
        with st.expander("‚ÑπÔ∏è Model Info"):
            model_info = get_model_info(selected_model)
            if model_info:
                st.json(model_info)
            else:
                st.info("‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• model ‡πÑ‡∏î‡πâ")

        st.markdown("---")
        
        # üÜï Processing Device Selector (‡∏á‡πà‡∏≤‡∏¢‡πÜ)
        st.subheader("‚ö° Processing Device")
        
        # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏°‡∏µ CUDA ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        try:
            import torch
            has_cuda = torch.cuda.is_available()
            
            if has_cuda:
                cuda_name = torch.cuda.get_device_name(0)
                st.success(f"‚úÖ GPU: {cuda_name}")
            else:
                st.warning("‚ö†Ô∏è GPU ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
        except ImportError:
            has_cuda = False
            st.warning("‚ö†Ô∏è PyTorch ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
        
        # ‡πÄ‡∏Å‡πá‡∏ö current device ‡πÉ‡∏ô session state
        if 'processing_device' not in st.session_state:
            st.session_state.processing_device = "GPU" if has_cuda else "CPU"
        
        # Radio button ‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢
        device_options = ["GPU (‡πÄ‡∏£‡πá‡∏ß)", "CPU (‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î)"] if has_cuda else ["CPU (‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î)"]
        device_labels = {
            "GPU (‡πÄ‡∏£‡πá‡∏ß)": "GPU",
            "CPU (‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î)": "CPU"
        }
        
        # ‡∏´‡∏≤ index ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
        current_label = f"{st.session_state.processing_device} ({'‡πÄ‡∏£‡πá‡∏ß' if st.session_state.processing_device == 'GPU' else '‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î'})"
        current_index = 0
        for i, opt in enumerate(device_options):
            if device_labels[opt] == st.session_state.processing_device:
                current_index = i
                break
        
        selected_device_label = st.radio(
            "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Device",
            device_options,
            index=current_index,
            help="""
            GPU (‡πÄ‡∏£‡πá‡∏ß) = ‡πÉ‡∏ä‡πâ GPU ‡∏ó‡∏±‡πâ‡∏á LLM ‡πÅ‡∏•‡∏∞ Embedding - ‡πÄ‡∏£‡πá‡∏ß‡∏™‡∏∏‡∏î! ‚ö°
            CPU (‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î) = ‡πÉ‡∏ä‡πâ CPU ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î - ‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤ ‡πÅ‡∏ï‡πà‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î VRAM
            """
        )
        
        selected_device = device_labels[selected_device_label]
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°
        if selected_device == "GPU":
            st.info("üí° ‡πÉ‡∏ä‡πâ GPU ‡∏ó‡∏±‡πâ‡∏á LLM ‡πÅ‡∏•‡∏∞ Embedding")
        else:
            st.info("üí° ‡πÉ‡∏ä‡πâ CPU ‡∏ó‡∏±‡πâ‡∏á LLM ‡πÅ‡∏•‡∏∞ Embedding")
        
        # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô device ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if selected_device != st.session_state.processing_device:
            st.session_state.processing_device = selected_device
            st.session_state.device_changed = True
            st.success(f"‚úÖ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô {selected_device}")
            st.info("üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÉ‡∏´‡∏°‡πà...")

        st.markdown("---")
        
        # üÜï Embedding Device Selector
        st.subheader("‚öôÔ∏è Embedding Device")
        
        # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏°‡∏µ CUDA ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        import torch
        has_cuda = torch.cuda.is_available()
        
        if has_cuda:
            cuda_name = torch.cuda.get_device_name(0)
            st.success(f"‚úÖ GPU Available: {cuda_name}")
            
            device_options = ["cuda", "cpu"]
            device_help = """
            cuda = ‡πÉ‡∏ä‡πâ GPU (‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ 5-10x) ‚ö°
            cpu = ‡πÉ‡∏ä‡πâ CPU (‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤ ‡πÅ‡∏ï‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏ó‡∏∏‡∏Å‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á)
            """
        else:
            st.warning("‚ö†Ô∏è No GPU detected")
            device_options = ["cpu"]
            device_help = "GPU ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ‡∏à‡∏∞‡πÉ‡∏ä‡πâ CPU ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô"
        
        # ‡πÄ‡∏Å‡πá‡∏ö current device ‡πÉ‡∏ô session state
        if 'embedding_device' not in st.session_state:
            st.session_state.embedding_device = "cuda" if has_cuda else "cpu"
        
        selected_device = st.radio(
            "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Device ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Embeddings",
            device_options,
            index=device_options.index(st.session_state.embedding_device),
            help=device_help
        )
        
        # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô device ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if selected_device != st.session_state.embedding_device:
            st.session_state.embedding_device = selected_device
            st.session_state.device_changed = True
            st.info(f"üîÑ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô {selected_device.upper()}")

        st.markdown("---")

        # üÜï RAG Type Selector (‡πÄ‡∏û‡∏¥‡πà‡∏° LightRAG)
        st.subheader("üîç RAG Type")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ LightRAG ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        from src.rag import is_lightrag_available
        lightrag_available = is_lightrag_available()
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á options
        rag_options = [
            "Naive RAG",
            "Contextual RAG",
            "Rerank RAG",
            "Hybrid RAG",
            "Query Rewrite RAG",
            "Multi-step RAG"
        ]
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° LightRAG ‡∏ñ‡πâ‡∏≤‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
        if lightrag_available:
            rag_options.append("LightRAG üåü")
        
        rag_type = st.selectbox(
            "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö RAG",
            rag_options,
            index=3,  # Default: Hybrid RAG
            help="""
            **Naive RAG**: ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô - Vector search (‡πÄ‡∏£‡πá‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)
            
            **Contextual RAG**: ‡πÄ‡∏ô‡πâ‡∏ô context ‡∏£‡∏≠‡∏ö‡πÜ
            
            **Rerank RAG**: ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏±‡∏î‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÉ‡∏´‡∏°‡πà
            
            **Hybrid RAG** üî•: BM25 (keyword) + Vector (semantic) - ‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢!
            
            **Query Rewrite RAG**: ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö
            
            **Multi-step RAG**: ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏≠‡∏ö‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏û‡∏≠
            
            **LightRAG** üåü: Graph-based RAG with Knowledge Graph
            - Entity & Relationship extraction
            - Multi-hop reasoning
            - ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô
            - ‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤ RAG ‡∏≠‡∏∑‡πà‡∏ô ‡πÅ‡∏ï‡πà‡πÅ‡∏°‡πà‡∏ô‡∏°‡∏≤‡∏Å
            """
        )
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ LightRAG
        if lightrag_available:
            st.success("‚úÖ LightRAG Available")
        else:
            st.info("üí° ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ LightRAG?")
            with st.expander("üì¶ Installation"):
                st.code("""
pip install lightrag-hku networkx
                """.strip(), language="bash")
                st.caption("‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡πâ‡∏ß restart Streamlit")
        
        st.markdown("---")
        
        # Document stats
        st.subheader("üìö Knowledge Base")
        doc_count = memory_handler.count_documents()
        st.metric("Total Chunks", doc_count)
        
        # ‚úÖ ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ LightRAG (‡∏ñ‡πâ‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å LightRAG)
        if rag_type == "LightRAG üåü":
            st.markdown("---")
            st.markdown("#### üåü LightRAG Graph Status")
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô LightRAG DB ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            lightrag_dir = Path("./data/lightrag_db")
            
            if lightrag_dir.exists() and any(lightrag_dir.glob("*")):
                # ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡πâ‡∏ß
                graph_files = list(lightrag_dir.glob("*"))
                st.success(f"‚úÖ Graph Ready ({len(graph_files)} files)")
                
                with st.expander("üìä View Graph Files"):
                    for f in graph_files:
                        try:
                            size = f.stat().st_size / 1024  # KB
                            st.text(f"  üìÑ {f.name} ({size:.1f} KB)")
                        except:
                            st.text(f"  üìÑ {f.name}")
            else:
                # ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• - ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô
                st.warning("‚ö†Ô∏è **LightRAG Database Empty**")
                st.info("""
**To use LightRAG:**
1. Keep "LightRAG üåü" selected
2. Upload documents below  
3. Wait for graph building
4. Start asking questions
                """)
            
            st.markdown("---")
        
        # Show document list
        sources = memory_handler.get_all_sources()
        docs_to_delete = []
        
        if sources:
            st.write(f"**Documents ({len(sources)}):**")
            
            with st.expander("üìÑ View Documents", expanded=False):
                for source in sources:
                    col1, col2 = st.columns([3, 1])
                    with col1:
                        st.text(f"üìÑ {source}")
                    with col2:
                        if st.button("üóëÔ∏è", key=f"delete_{source}", help="‡∏•‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ô‡∏µ‡πâ"):
                            docs_to_delete.append(source)
        else:
            st.info("‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£")
        
        # File uploader
        st.subheader("üì§ Upload Documents")
        uploaded_files = st.file_uploader(
            "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå",
            type=['txt', 'pdf', 'docx', 'md', 'json'],
            accept_multiple_files=True,
            help="‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö: TXT, PDF, DOCX, MD, JSON"
        )
        
        # Options
        st.subheader("üõ†Ô∏è Options")
        
        # Debug Mode Toggle
        debug_mode = st.checkbox("üîç Debug Mode", value=False, help="‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• debug")
        
        clear_chat = st.button("üóëÔ∏è Clear Chat")
        clear_memory = st.button(
            "üóëÔ∏è Clear All Documents", 
            help="‡∏•‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î",
            type="secondary"
        )
        
        # About
        st.markdown("---")
        st.markdown("""
        **Personal AI Assistant**
        
        - ü§ñ Multi-Model Support
        - üíæ Persistent Memory
        - üìö Advanced RAG (7 types)
        - üåü LightRAG Support
        - üñºÔ∏è Image Support
        - üîí 100% Private
        
        [GitHub](https://github.com/6amdev/personal-ai-assistant)
        """)
        
        return clear_chat, clear_memory, uploaded_files, docs_to_delete, debug_mode, rag_type, selected_model, selected_device, selected_device


def chat_interface(llm_handler, memory_handler, rag_system=None):
    """
    Main chat interface with RAG and Images (Enhanced)
    
    Args:
        llm_handler: LLM Handler
        memory_handler: Memory Handler
        rag_system: RAG System (‡πÉ‡∏´‡∏°‡πà!) üÜï
    """
    
    # üÜï ‡∏™‡∏£‡πâ‡∏≤‡∏á RAG system (‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ)
    if rag_system is None:
        from src.rag import NaiveRAG
        if "rag_system" not in st.session_state:
            st.session_state.rag_system = NaiveRAG(llm_handler, memory_handler)
        rag_system = st.session_state.rag_system
    
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # Display chat history
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            
            # ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
            if "images" in message and message["images"]:
                display_images(message["images"])
    
    # Chat input
    if prompt := st.chat_input("‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°..."):
        # Add user message
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Get AI response with RAG
        with st.chat_message("assistant"):
            with st.spinner("ü§î ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡∏¥‡∏î..."):
                try:
                    # üî• ‡πÉ‡∏ä‡πâ RAG System ‡πÅ‡∏ó‡∏ô!
                    result = rag_system.query(prompt, k=3)
                    
                    # ‡∏î‡∏∂‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏à‡∏≤‡∏Å context
                    images = ImageHandler.extract_images_from_context(result['context']) if result['context'] else []
                    
                    # Debug info (‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏¥‡∏î debug mode)
                    debug_mode = st.session_state.get("debug_mode", False)
                    if debug_mode:
                        with st.expander("üîç Debug Info", expanded=True):
                            st.write(f"**RAG Type:** {result['rag_type']}")
                            st.write(f"**Model:** {llm_handler.get_model_name()}")
                            st.write(f"**Context length:** {len(result['context']) if result['context'] else 0} chars")
                            st.write(f"**Sources:** {', '.join(result['sources'])}")
                            st.write(f"**Found images:** {len(images)}")
                            if images:
                                st.json(images)
                            if result['context']:
                                st.text_area("Context Preview", result['context'][:500] + "..." if len(result['context']) > 500 else result['context'], height=200)
                    
                    # ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
                    st.markdown(result['answer'])
                    
                    # ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
                    if images:
                        st.markdown("---")
                        st.markdown("**üñºÔ∏è ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á:**")
                        display_images(images)
                    
                    # Show context used
                    if result['context']:
                        with st.expander("üìö ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á"):
                            st.text(result['context'][:1000] + "..." if len(result['context']) > 1000 else result['context'])
                    
                    # Add to history
                    st.session_state.messages.append({
                        "role": "assistant", 
                        "content": result['answer'],
                        "images": images if images else []
                    })
                    
                except Exception as e:
                    st.error(f"‚ùå Error: {str(e)}")
                    st.info("üí° ‡∏•‡∏≠‡∏á‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô RAG type ‡∏´‡∏£‡∏∑‡∏≠ upload ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÉ‡∏´‡∏°‡πà")


def display_images(images: list):
    """
    ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÅ‡∏ö‡∏ö inline thumbnail ‡∏û‡∏£‡πâ‡∏≠‡∏° Lightbox
    
    Args:
        images: List of image dicts [{"type": "url/local/base64", "data": "...", "caption": "..."}]
    """
    if not images:
        return
    
    import hashlib
    import time
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á unique ID
    lightbox_id = hashlib.md5(str(time.time()).encode()).hexdigest()[:8]
    
    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
    processed_images = []
    for img_info in images:
        img_type = img_info.get("type", "url")
        img_data = img_info.get("data", "")
        caption = img_info.get("caption", "")
        
        try:
            if img_type == "url":
                processed_images.append({
                    "src": img_data,
                    "caption": caption or "‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û"
                })
            elif img_type == "local":
                if Path(img_data).exists():
                    with open(img_data, "rb") as f:
                        img_bytes = f.read()
                        img_b64 = base64.b64encode(img_bytes).decode()
                        img_ext = Path(img_data).suffix[1:]
                        data_url = f"data:image/{img_ext};base64,{img_b64}"
                        processed_images.append({
                            "src": data_url,
                            "caption": caption or "‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û"
                        })
            elif img_type == "base64":
                data_url = f"data:image/png;base64,{img_data}"
                processed_images.append({
                    "src": data_url,
                    "caption": caption or "‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û"
                })
        except Exception as e:
            st.warning(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û: {e}")
    
    if not processed_images:
        st.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÑ‡∏î‡πâ")
        return
    
    # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô JSON
    images_json = json.dumps(processed_images)
    
    # CSS ‡πÅ‡∏•‡∏∞ JavaScript
    st.markdown(f"""
    <style>
    .lightbox-{lightbox_id} {{
        display: none;
        position: fixed;
        z-index: 9999;
        padding-top: 50px;
        left: 0;
        top: 0;
        width: 100%;
        height: 100%;
        overflow: auto;
        background-color: rgba(0,0,0,0.95);
    }}
    
    .lightbox-content-{lightbox_id} {{
        margin: auto;
        display: block;
        max-width: 90%;
        max-height: 85vh;
        object-fit: contain;
        animation: zoom-{lightbox_id} 0.3s;
    }}
    
    @keyframes zoom-{lightbox_id} {{
        from {{transform: scale(0.8); opacity: 0;}}
        to {{transform: scale(1); opacity: 1;}}
    }}
    
    .lightbox-close-{lightbox_id} {{
        position: absolute;
        top: 20px;
        right: 40px;
        color: #f1f1f1;
        font-size: 45px;
        font-weight: bold;
        cursor: pointer;
        transition: 0.3s;
        z-index: 10000;
    }}
    
    .lightbox-close-{lightbox_id}:hover {{
        color: #ff4444;
    }}
    
    .lightbox-caption-{lightbox_id} {{
        margin: auto;
        display: block;
        width: 80%;
        max-width: 700px;
        text-align: center;
        color: #fff;
        padding: 15px 0;
        font-size: 18px;
    }}
    
    .lightbox-arrow-{lightbox_id} {{
        cursor: pointer;
        position: absolute;
        top: 50%;
        width: auto;
        padding: 20px;
        margin-top: -50px;
        color: white;
        font-weight: bold;
        font-size: 35px;
        transition: 0.3s;
        user-select: none;
        background-color: rgba(0, 0, 0, 0.6);
        border-radius: 5px;
        z-index: 10000;
    }}
    
    .lightbox-arrow-{lightbox_id}:hover {{
        background-color: rgba(0, 0, 0, 0.9);
    }}
    
    .arrow-left-{lightbox_id} {{
        left: 25px;
    }}
    
    .arrow-right-{lightbox_id} {{
        right: 25px;
    }}
    
    .lightbox-counter-{lightbox_id} {{
        position: absolute;
        top: 25px;
        left: 50%;
        transform: translateX(-50%);
        color: #f1f1f1;
        font-size: 20px;
        background-color: rgba(0, 0, 0, 0.7);
        padding: 10px 20px;
        border-radius: 5px;
        z-index: 10000;
    }}
    
    .image-grid-{lightbox_id} {{
        display: grid;
        grid-template-columns: repeat(auto-fill, minmax(200px, 1fr));
        gap: 15px;
        margin: 15px 0;
    }}
    
    .thumbnail-container-{lightbox_id} {{
        position: relative;
        overflow: hidden;
        border-radius: 12px;
        box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        transition: all 0.3s ease;
        cursor: pointer;
        background: #f5f5f5;
    }}
    
    .thumbnail-container-{lightbox_id}:hover {{
        transform: translateY(-5px);
        box-shadow: 0 8px 16px rgba(0,0,0,0.2);
    }}
    
    .thumbnail-{lightbox_id} {{
        width: 100%;
        height: 200px;
        object-fit: cover;
        display: block;
    }}
    
    .thumbnail-caption-{lightbox_id} {{
        padding: 10px;
        text-align: center;
        color: #333;
        font-size: 14px;
        background: white;
        border-top: 1px solid #eee;
        min-height: 45px;
        display: flex;
        align-items: center;
        justify-content: center;
    }}
    
    .thumbnail-overlay-{lightbox_id} {{
        position: absolute;
        top: 0;
        left: 0;
        right: 0;
        bottom: 45px;
        background: rgba(0,0,0,0.5);
        display: flex;
        align-items: center;
        justify-content: center;
        opacity: 0;
        transition: opacity 0.3s;
        color: white;
        font-size: 40px;
    }}
    
    .thumbnail-container-{lightbox_id}:hover .thumbnail-overlay-{lightbox_id} {{
        opacity: 1;
    }}
    </style>
    
    <script>
    (function() {{
        const images_{lightbox_id} = {images_json};
        let currentImageIndex_{lightbox_id} = 0;
        
        window.openLightbox_{lightbox_id} = function(index) {{
            currentImageIndex_{lightbox_id} = index;
            const img = images_{lightbox_id}[index];
            
            const lightbox = document.getElementById('lightbox-{lightbox_id}');
            const lightboxImg = document.getElementById('lightbox-img-{lightbox_id}');
            const caption = document.getElementById('lightbox-caption-{lightbox_id}');
            const counter = document.getElementById('lightbox-counter-{lightbox_id}');
            const arrowLeft = document.getElementById('arrow-left-{lightbox_id}');
            const arrowRight = document.getElementById('arrow-right-{lightbox_id}');
            
            if (lightbox && lightboxImg) {{
                lightbox.style.display = 'block';
                lightboxImg.src = img.src;
                if (caption) caption.innerHTML = img.caption || '';
                if (counter) counter.innerHTML = (index + 1) + ' / ' + images_{lightbox_id}.length;
                
                if (arrowLeft) arrowLeft.style.display = index > 0 ? 'block' : 'none';
                if (arrowRight) arrowRight.style.display = index < images_{lightbox_id}.length - 1 ? 'block' : 'none';
            }}
        }}
        
        window.closeLightbox_{lightbox_id} = function() {{
            const lightbox = document.getElementById('lightbox-{lightbox_id}');
            if (lightbox) lightbox.style.display = 'none';
        }}
        
        window.changeImage_{lightbox_id} = function(direction) {{
            let newIndex = currentImageIndex_{lightbox_id} + direction;
            if (newIndex >= 0 && newIndex < images_{lightbox_id}.length) {{
                window.openLightbox_{lightbox_id}(newIndex);
            }}
        }}
        
        setTimeout(function() {{
            const lightbox = document.getElementById('lightbox-{lightbox_id}');
            if (lightbox) {{
                lightbox.onclick = function(event) {{
                    if (event.target.id === 'lightbox-{lightbox_id}') {{
                        window.closeLightbox_{lightbox_id}();
                    }}
                }}
            }}
            
            const keyHandler_{lightbox_id} = function(event) {{
                const lightbox = document.getElementById('lightbox-{lightbox_id}');
                if (lightbox && lightbox.style.display === 'block') {{
                    if (event.key === 'Escape') {{
                        window.closeLightbox_{lightbox_id}();
                    }} else if (event.key === 'ArrowRight') {{
                        window.changeImage_{lightbox_id}(1);
                    }} else if (event.key === 'ArrowLeft') {{
                        window.changeImage_{lightbox_id}(-1);
                    }}
                }}
            }}
            
            document.removeEventListener('keydown', keyHandler_{lightbox_id});
            document.addEventListener('keydown', keyHandler_{lightbox_id});
        }}, 100);
    }})();
    </script>
    """, unsafe_allow_html=True)
    
    # Lightbox container
    st.markdown(f"""
    <div id="lightbox-{lightbox_id}" class="lightbox-{lightbox_id}">
        <span class="lightbox-close-{lightbox_id}" onclick="closeLightbox_{lightbox_id}()">&times;</span>
        <div class="lightbox-counter-{lightbox_id}" id="lightbox-counter-{lightbox_id}">1 / 1</div>
        <span class="lightbox-arrow-{lightbox_id} arrow-left-{lightbox_id}" id="arrow-left-{lightbox_id}" 
              onclick="changeImage_{lightbox_id}(-1)">&#10094;</span>
        <img class="lightbox-content-{lightbox_id}" id="lightbox-img-{lightbox_id}" alt="Image">
        <span class="lightbox-arrow-{lightbox_id} arrow-right-{lightbox_id}" id="arrow-right-{lightbox_id}" 
              onclick="changeImage_{lightbox_id}(1)">&#10095;</span>
        <div class="lightbox-caption-{lightbox_id}" id="lightbox-caption-{lightbox_id}"></div>
    </div>
    """, unsafe_allow_html=True)
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏π‡∏õ‡πÄ‡∏õ‡πá‡∏ô Grid
    html_content = f'<div class="image-grid-{lightbox_id}">'
    
    for idx, img in enumerate(processed_images):
        caption_escaped = img['caption'].replace("'", "\\'").replace('"', '&quot;')
        src_escaped = img['src'].replace("'", "\\'")
        
        html_content += f"""
        <div class="thumbnail-container-{lightbox_id}" onclick="openLightbox_{lightbox_id}({idx})">
            <img src="{src_escaped}" class="thumbnail-{lightbox_id}" alt="{caption_escaped}" loading="lazy" onerror="this.src='https://via.placeholder.com/200x200?text=Image+Error'">
            <div class="thumbnail-overlay-{lightbox_id}">üîç</div>
            <div class="thumbnail-caption-{lightbox_id}">{img['caption']}</div>
        </div>
        """
    
    html_content += '</div>'
    st.markdown(html_content, unsafe_allow_html=True)


def clear_chat_history():
    """Clear chat history"""
    st.session_state.messages = []
    st.success("‚úÖ ‡∏•‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÅ‡∏•‡πâ‡∏ß!")