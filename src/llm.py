"""LLM Handler - With Device Support"""
import sys
import os
from pathlib import Path

root_dir = Path(__file__).parent.parent
sys.path.insert(0, str(root_dir))

from langchain_community.llms import Ollama

try:
    from config import LLM_MODEL, LLM_TEMPERATURE, SYSTEM_PROMPT
except ImportError:
    LLM_MODEL = "llama3.1:8b"
    LLM_TEMPERATURE = 0.7
    SYSTEM_PROMPT = """à¸„à¸¸à¸“à¹€à¸›à¹‡à¸™ AI à¸œà¸¹à¹‰à¸Šà¹ˆà¸§à¸¢à¸—à¸µà¹ˆà¸‰à¸¥à¸²à¸”à¹à¸¥à¸°à¹€à¸›à¹‡à¸™à¸¡à¸´à¸•à¸£
    
à¸à¸Žà¸ªà¸³à¸„à¸±à¸:
1. à¸•à¸­à¸šà¹€à¸›à¹‡à¸™à¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¹€à¸ªà¸¡à¸­
2. à¹ƒà¸Šà¹‰ "à¸„à¸£à¸±à¸š/à¸„à¹ˆà¸°" à¹ƒà¸«à¹‰à¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡
3. à¸•à¸­à¸šà¸ªà¸±à¹‰à¸™ à¸à¸£à¸°à¸Šà¸±à¸š à¸Šà¸±à¸”à¹€à¸ˆà¸™
4. à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸£à¸¹à¹‰ à¸šà¸­à¸à¸•à¸£à¸‡à¹†"""


class LLMHandler:
    def __init__(self, model_name: str = None, device: str = "GPU"):
        """
        Initialize LLM Handler
        
        Args:
            model_name: à¸Šà¸·à¹ˆà¸­ model (à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸£à¸°à¸šà¸¸à¸ˆà¸°à¹ƒà¸Šà¹‰à¸ˆà¸²à¸ config)
            device: "GPU" à¸«à¸£à¸·à¸­ "CPU"
        """
        self.model_name = model_name or LLM_MODEL
        self.device = device.upper()
        
        print(f"ðŸ¤– Initializing LLM: {self.model_name}")
        print(f"   Device: {self.device}")
        
        # ðŸ”¥ Ollama à¹ƒà¸Šà¹‰ GPU à¹‚à¸”à¸¢ default
        # à¸–à¹‰à¸²à¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸šà¸±à¸‡à¸„à¸±à¸šà¹ƒà¸Šà¹‰ CPU à¹ƒà¸«à¹‰à¸•à¸±à¹‰à¸‡ env var
        if self.device == "CPU":
            os.environ['OLLAMA_NUM_GPU'] = '0'
            print("   âš ï¸ LLM will use CPU only")
        else:
            # à¹ƒà¸Šà¹‰ GPU (à¸¥à¸š env var à¸–à¹‰à¸²à¸¡à¸µ)
            if 'OLLAMA_NUM_GPU' in os.environ:
                del os.environ['OLLAMA_NUM_GPU']
            print("   âœ… LLM will use GPU")
        
        try:
            self.llm = Ollama(
                model=self.model_name,
                temperature=LLM_TEMPERATURE,
                system=SYSTEM_PROMPT
            )
            print(f"âœ… LLM ready: {self.model_name} on {self.device}")
        except Exception as e:
            print(f"âŒ Error initializing LLM: {e}")
            raise
    
    def generate(self, prompt: str) -> str:
        """Generate response from LLM"""
        try:
            return self.llm.invoke(prompt)
        except Exception as e:
            return f"à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸žà¸¥à¸²à¸”: {str(e)}"
    
    def get_model_name(self) -> str:
        """Get current model name"""
        return self.model_name


if __name__ == "__main__":
    print("Testing LLM Handler...")
    
    # Test GPU
    print("\n=== Test GPU ===")
    llm_gpu = LLMHandler(device="GPU")
    print(f"âœ… Model: {llm_gpu.get_model_name()}")
    response = llm_gpu.generate("1+1=?")
    print(f"Response: {response[:100]}")
    
    # Test CPU
    print("\n=== Test CPU ===")
    llm_cpu = LLMHandler(device="CPU")
    print(f"âœ… Model: {llm_cpu.get_model_name()}")
    response2 = llm_cpu.generate("à¸ªà¸§à¸±à¸ªà¸”à¸µ")
    print(f"Response: {response2[:100]}")