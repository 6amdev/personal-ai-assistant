"""LLM Handler"""
import sys
from pathlib import Path

# à¹€à¸à¸´à¹ˆà¸¡ root folder à¹€à¸‚à¹‰à¸² path
root_dir = Path(__file__).parent.parent
sys.path.insert(0, str(root_dir))

from langchain_community.llms import Ollama

try:
    from config import LLM_MODEL, LLM_TEMPERATURE, SYSTEM_PROMPT
except ImportError:
    LLM_MODEL = "llama3.1:8b"
    LLM_TEMPERATURE = 0.7
    SYSTEM_PROMPT = """à¸„à¸¸à¸“à¹€à¸›à¹‡à¸™ AI à¸œà¸¹à¹‰à¸Šà¹ˆà¸§à¸¢à¸—à¸µà¹ˆà¸‰à¸¥à¸²à¸”à¹à¸¥à¸°à¹€à¸›à¹‡à¸™à¸¡à¸´à¸•à¸£
    
à¸à¸à¸ªà¸³à¸„à¸±à¸:
1. à¸•à¸­à¸šà¹€à¸›à¹‡à¸™à¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¹€à¸ªà¸¡à¸­
2. à¹ƒà¸Šà¹‰ "à¸„à¸£à¸±à¸š/à¸„à¹ˆà¸°" à¹ƒà¸«à¹‰à¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡
3. à¸•à¸­à¸šà¸ªà¸±à¹‰à¸™ à¸à¸£à¸°à¸Šà¸±à¸š à¸Šà¸±à¸”à¹€à¸ˆà¸™
4. à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸£à¸¹à¹‰ à¸šà¸­à¸à¸•à¸£à¸‡à¹†"""


class LLMHandler:
    def __init__(self):
        print(f"ğŸ¤– Initializing LLM: {LLM_MODEL}")
        self.llm = Ollama(
            model=LLM_MODEL,
            temperature=LLM_TEMPERATURE,
            system=SYSTEM_PROMPT
        )
    
    def generate(self, prompt: str) -> str:
        """Generate response from LLM"""
        try:
            # à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¸ˆà¸²à¸ self.llm(prompt) à¹€à¸›à¹‡à¸™ invoke
            return self.llm.invoke(prompt)  # â† à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¸•à¸£à¸‡à¸™à¸µà¹‰!
        except Exception as e:
            return f"à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸à¸¥à¸²à¸”: {str(e)}"
    
    def get_model_name(self) -> str:
        return LLM_MODEL


if __name__ == "__main__":
    print("Testing LLM Handler...")
    llm = LLMHandler()
    print(f"âœ… Model: {llm.get_model_name()}")
    
    # à¸—à¸”à¸ªà¸­à¸šà¸ à¸²à¸©à¸²à¹„à¸—à¸¢
    print("\nğŸ“ à¸—à¸”à¸ªà¸­à¸š 1:")
    response = llm.generate("1+1=?")
    print(f"Q: 1+1=?")
    print(f"A: {response}")
    
    print("\nğŸ“ à¸—à¸”à¸ªà¸­à¸š 2:")
    response2 = llm.generate("AI à¸„à¸·à¸­à¸­à¸°à¹„à¸£")
    print(f"Q: AI à¸„à¸·à¸­à¸­à¸°à¹„à¸£")
    print(f"A: {response2}")