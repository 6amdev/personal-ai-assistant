# ğŸ’¬ Personal AI Assistant

> AI Chatbot à¸ªà¹ˆà¸§à¸™à¸•à¸±à¸§à¸—à¸µà¹ˆà¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰à¸ˆà¸²à¸à¹€à¸­à¸à¸ªà¸²à¸£à¸‚à¸­à¸‡à¸„à¸¸à¸“ - à¸Ÿà¸£à¸µ 100% à¸£à¸±à¸™à¸šà¸™à¹€à¸„à¸£à¸·à¹ˆà¸­à¸‡à¹€à¸­à¸‡!

[![Python](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Ollama](https://img.shields.io/badge/ollama-required-orange.svg)](https://ollama.com)
[![Made by](https://img.shields.io/badge/made%20by-6amdev-red.svg)](https://github.com/6amdev)

## âœ¨ Features

- ğŸ¤– **Multi-Model Support** - à¹€à¸¥à¸·à¸­à¸à¹ƒà¸Šà¹‰ LLM à¸«à¸¥à¸²à¸¢à¸•à¸±à¸§ (Llama, Qwen, DeepSeek, etc.)
- ğŸ“š **Advanced RAG** - à¸£à¸°à¸šà¸š RAG 6 à¹à¸šà¸š (Naive, Contextual, Rerank, Hybrid, Query Rewrite, Multi-step)
- ğŸ’¾ **Persistent Memory** - à¸ˆà¸³à¸šà¸—à¸ªà¸™à¸—à¸™à¸²à¹à¸¥à¸°à¹€à¸­à¸à¸ªà¸²à¸£à¸–à¸²à¸§à¸£à¸”à¹‰à¸§à¸¢ ChromaDB
- ğŸ–¼ï¸ **Image Support** - à¹à¸ªà¸”à¸‡à¸£à¸¹à¸›à¸ à¸²à¸à¸ˆà¸²à¸à¹€à¸­à¸à¸ªà¸²à¸£
- ğŸ“„ **Multi-Format** - à¸£à¸­à¸‡à¸£à¸±à¸š TXT, PDF, DOCX, JSON, MD
- ğŸ‡¹ğŸ‡­ **Thai Language** - à¸£à¸­à¸‡à¸£à¸±à¸šà¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¹„à¸”à¹‰à¸”à¸µ (à¹‚à¸”à¸¢à¹€à¸‰à¸à¸²à¸° Qwen 2.5)
- ğŸ”’ **100% Private** - à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹„à¸¡à¹ˆà¸­à¸­à¸à¸ˆà¸²à¸à¹€à¸„à¸£à¸·à¹ˆà¸­à¸‡
- ğŸ†“ **Free & Open Source** - à¹ƒà¸Šà¹‰à¸Ÿà¸£à¸µ à¸›à¸£à¸±à¸šà¹à¸•à¹ˆà¸‡à¹„à¸”à¹‰
- âš¡ **Fast Embeddings** - Sentence Transformers (5-10x à¹€à¸£à¹‡à¸§à¸à¸§à¹ˆà¸² Ollama embeddings)
- ğŸ¨ **Modern UI** - Interface à¸ªà¸§à¸¢à¸‡à¸²à¸¡à¸”à¹‰à¸§à¸¢ Streamlit

---

## ğŸš€ Quick Start

### Installation

```bash
# 1. Clone repository
git clone https://github.com/6amdev/personal-ai-assistant.git
cd personal-ai-assistant

# 2. Install dependencies
pip install -r requirements.txt

# 3. Install Ollama (if not installed)
curl -fsSL https://ollama.com/install.sh | sh

# 4. Pull AI models
ollama pull llama3.1:8b      # Default
ollama pull qwen2.5:7b        # Recommended for Thai
ollama pull deepseek-r1:8b    # Good for reasoning

# 5. Run the app
streamlit run app.py
```

à¹€à¸›à¸´à¸” browser à¸—à¸µà¹ˆ `http://localhost:8501` ğŸ‰

---

## ğŸ“– Usage

### ğŸ’¬ Chat
à¸à¸´à¸¡à¸à¹Œà¸„à¸³à¸–à¸²à¸¡ â†’ AI à¸•à¸­à¸š (à¹ƒà¸Šà¹‰à¸„à¸§à¸²à¸¡à¸£à¸¹à¹‰à¸—à¸±à¹ˆà¸§à¹„à¸›)

### ğŸ“š Upload Documents
1. Sidebar â†’ **ğŸ“¤ Upload Documents**
2. à¹€à¸¥à¸·à¸­à¸à¹„à¸Ÿà¸¥à¹Œ (TXT, PDF, DOCX, JSON, MD)
3. à¸£à¸­à¸£à¸°à¸šà¸šà¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰à¹€à¸­à¸à¸ªà¸²à¸£ (~10-30s)
4. à¸–à¸²à¸¡à¸„à¸³à¸–à¸²à¸¡à¹€à¸à¸µà¹ˆà¸¢à¸§à¸à¸±à¸šà¹€à¸­à¸à¸ªà¸²à¸£ â†’ AI à¸•à¸­à¸šà¸ˆà¸²à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¸­à¸±à¸›à¹‚à¸«à¸¥à¸”

### ğŸ¤– Select Models
Sidebar â†’ **ğŸ¤– LLM Model** â†’ à¹€à¸¥à¸·à¸­à¸ model à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£

**à¹à¸™à¸°à¸™à¸³:**
- **Qwen 2.5:7b** - à¸”à¸µà¸—à¸µà¹ˆà¸ªà¸¸à¸”à¸ªà¸³à¸«à¸£à¸±à¸šà¸ à¸²à¸©à¸²à¹„à¸—à¸¢ ğŸ”¥
- **DeepSeek R1:8b** - à¹€à¸à¹ˆà¸‡à¸„à¸“à¸´à¸•à¸¨à¸²à¸ªà¸•à¸£à¹Œà¹à¸¥à¸° reasoning
- **Llama 3.1:8b** - Balanced, à¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¸—à¸±à¹ˆà¸§à¹„à¸›

### ğŸ” Select RAG Type
Sidebar â†’ **ğŸ” RAG Type** â†’ à¹€à¸¥à¸·à¸­à¸à¹à¸šà¸š RAG

**à¹à¸™à¸°à¸™à¸³:**
- **Hybrid RAG** ğŸ”¥ - à¸”à¸µà¸—à¸µà¹ˆà¸ªà¸¸à¸”à¸ªà¸³à¸«à¸£à¸±à¸šà¸ à¸²à¸©à¸²à¹„à¸—à¸¢ (BM25 + Vector)
- **Naive RAG** - à¹€à¸£à¹‡à¸§à¸—à¸µà¹ˆà¸ªà¸¸à¸” (à¸à¸·à¹‰à¸™à¸à¸²à¸™)
- **Multi-step RAG** - à¹€à¸«à¸¡à¸²à¸°à¸à¸±à¸šà¸„à¸³à¸–à¸²à¸¡à¸‹à¸±à¸šà¸‹à¹‰à¸­à¸™

### ğŸ—‘ï¸ Manage Documents
Sidebar â†’ **ğŸ“„ View Documents** â†’ à¸„à¸¥à¸´à¸ **ğŸ—‘ï¸** à¹€à¸à¸·à¹ˆà¸­à¸¥à¸šà¸—à¸µà¸¥à¸°à¹„à¸Ÿà¸¥à¹Œ

---

## ğŸ› ï¸ Tech Stack

### Core Technologies

| Technology | Purpose | Why We Use It |
|-----------|---------|---------------|
| **[Streamlit](https://streamlit.io)** | Web UI Framework | à¸ªà¸£à¹‰à¸²à¸‡ web app à¸”à¹‰à¸§à¸¢ Python à¸‡à¹ˆà¸²à¸¢à¹† à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¹€à¸‚à¸µà¸¢à¸™ HTML/CSS |
| **[LangChain](https://langchain.com)** | LLM Framework | à¹€à¸Šà¸·à¹ˆà¸­à¸¡à¸•à¹ˆà¸­ LLM, Memory, à¹à¸¥à¸° Tools à¹€à¸‚à¹‰à¸²à¸”à¹‰à¸§à¸¢à¸à¸±à¸™ |
| **[Llama 3.1 8B](https://ai.meta.com/llama/)** | Language Model | Open source LLM à¸ˆà¸²à¸ Meta à¸‰à¸¥à¸²à¸”à¹à¸¥à¸°à¸£à¸­à¸‡à¸£à¸±à¸šà¸ à¸²à¸©à¸²à¹„à¸—à¸¢ |
| **[Qwen 2.5](https://qwenlm.github.io/)** | Language Model | LLM à¸ˆà¸²à¸ Alibaba à¸”à¸µà¹€à¸¢à¸µà¹ˆà¸¢à¸¡à¸à¸±à¸šà¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¹à¸¥à¸°à¸«à¸¥à¸²à¸¢à¸ à¸²à¸©à¸² |
| **[DeepSeek R1](https://www.deepseek.com/)** | Language Model | à¹€à¸à¹ˆà¸‡à¸”à¹‰à¸²à¸™ reasoning à¹à¸¥à¸°à¸„à¸“à¸´à¸•à¸¨à¸²à¸ªà¸•à¸£à¹Œ |
| **[Ollama](https://ollama.com)** | LLM Runtime | à¸£à¸±à¸™ LLM à¸šà¸™à¹€à¸„à¸£à¸·à¹ˆà¸­à¸‡à¹€à¸£à¸²à¹„à¸”à¹‰ à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¹ƒà¸Šà¹‰ API |
| **[ChromaDB](https://trychroma.com)** | Vector Database | à¹€à¸à¹‡à¸š embeddings à¸ªà¸³à¸«à¸£à¸±à¸šà¸„à¹‰à¸™à¸«à¸²à¹€à¸­à¸à¸ªà¸²à¸£à¸—à¸µà¹ˆà¹€à¸à¸µà¹ˆà¸¢à¸§à¸‚à¹‰à¸­à¸‡ |
| **[Sentence Transformers](https://www.sbert.net/)** | Embeddings | à¹à¸›à¸¥à¸‡à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¹€à¸›à¹‡à¸™ vectors (à¹€à¸£à¹‡à¸§à¸à¸§à¹ˆà¸² Ollama 5-10x) |
| **Python 3.9+** | Backend Language | à¸ à¸²à¸©à¸²à¸«à¸¥à¸±à¸à¸‚à¸­à¸‡à¹‚à¸›à¸£à¹€à¸ˆà¸à¸•à¹Œ |

### Document Processing

| Library | Purpose |
|---------|---------|
| **pypdf** | à¸­à¹ˆà¸²à¸™à¹„à¸Ÿà¸¥à¹Œ PDF |
| **python-docx** | à¸­à¹ˆà¸²à¸™à¹„à¸Ÿà¸¥à¹Œ DOCX (Word) |
| **Pillow** | à¸ˆà¸±à¸”à¸à¸²à¸£à¸£à¸¹à¸›à¸ à¸²à¸ |

### AI Components

**RAG (Retrieval-Augmented Generation):**

à¸£à¸°à¸šà¸šà¸¡à¸µ 6 à¹à¸šà¸šà¹ƒà¸«à¹‰à¹€à¸¥à¸·à¸­à¸:

1. **Naive RAG** - à¸à¸·à¹‰à¸™à¸à¸²à¸™ à¸„à¹‰à¸™à¸«à¸²à¸”à¹‰à¸§à¸¢ vector similarity
2. **Contextual RAG** - à¹€à¸™à¹‰à¸™à¸à¸²à¸£à¹ƒà¸Šà¹‰ context à¸£à¸­à¸šà¹†
3. **Rerank RAG** - à¸„à¹‰à¸™à¸«à¸²à¹à¸¥à¹‰à¸§à¸ˆà¸±à¸”à¸­à¸±à¸™à¸”à¸±à¸šà¹ƒà¸«à¸¡à¹ˆ
4. **Hybrid RAG** ğŸ”¥ - à¸£à¸§à¸¡ BM25 (keyword) + Vector (semantic) - à¸”à¸µà¸—à¸µà¹ˆà¸ªà¸¸à¸”à¸ªà¸³à¸«à¸£à¸±à¸šà¸ à¸²à¸©à¸²à¹„à¸—à¸¢!
5. **Query Rewrite RAG** - à¹€à¸‚à¸µà¸¢à¸™à¸„à¸³à¸–à¸²à¸¡à¹ƒà¸«à¸¡à¹ˆà¸«à¸¥à¸²à¸¢à¹à¸šà¸š
6. **Multi-step RAG** - à¸„à¹‰à¸™à¸«à¸²à¸«à¸¥à¸²à¸¢à¸£à¸­à¸šà¸–à¹‰à¸²à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹„à¸¡à¹ˆà¸à¸­

**Embeddings:**
- à¹ƒà¸Šà¹‰ **Sentence Transformers** (paraphrase-multilingual-MiniLM-L12-v2)
- à¹à¸›à¸¥à¸‡à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¹€à¸›à¹‡à¸™ vectors (384 dimensions)
- à¹€à¸à¹‡à¸šà¹ƒà¸™ ChromaDB à¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸²à¸£à¸„à¹‰à¸™à¸«à¸²à¹à¸šà¸š semantic
- à¸£à¸­à¸‡à¸£à¸±à¸šà¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¹à¸¥à¸° 50+ à¸ à¸²à¸©à¸²

**Memory:**
- **Short-term:** à¸ˆà¸³à¸šà¸—à¸ªà¸™à¸—à¸™à¸²à¸›à¸±à¸ˆà¸ˆà¸¸à¸šà¸±à¸™ (ConversationBufferMemory)
- **Long-term:** à¸ˆà¸³à¹€à¸­à¸à¸ªà¸²à¸£à¸–à¸²à¸§à¸£ (ChromaDB)

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           User Interface (Streamlit)                    â”‚
â”‚  - Chat input/output                                    â”‚
â”‚  - Model selector (Llama/Qwen/DeepSeek)                â”‚
â”‚  - RAG type selector (6 types)                         â”‚
â”‚  - File uploader                                        â”‚
â”‚  - Document management UI                               â”‚
â”‚  - Image viewer (Lightbox)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Application Layer (LangChain + Custom RAG)         â”‚
â”‚  - Multi-model LLM handler                              â”‚
â”‚  - RAG implementations (6 types)                        â”‚
â”‚  - Prompt engineering                                   â”‚
â”‚  - Context retrieval                                    â”‚
â”‚  - Response generation                                  â”‚
â”‚  - Document processing (TXT/PDF/DOCX/JSON/MD)          â”‚
â”‚  - Image extraction                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ’¾ Memory (ChromaDB) â”‚  â”‚  ğŸ¤– LLM (Ollama)            â”‚
â”‚  - Vector Database    â”‚  â”‚  - Llama 3.1/3.2 (Meta)     â”‚
â”‚  - Sentence Trans.    â”‚  â”‚  - Qwen 2.5 (Alibaba)       â”‚
â”‚  - 384D Embeddings    â”‚  â”‚  - DeepSeek R1              â”‚
â”‚  - Semantic Search    â”‚  â”‚  - Dynamic switching        â”‚
â”‚  - Persistence        â”‚  â”‚  - GPU/CPU support          â”‚
â”‚  - Multi-language     â”‚  â”‚  - Local runtime            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

**1. Document Upload Flow:**
```
User uploads document
    â†“
Document Processor
    â”œâ”€ PDF â†’ pypdf
    â”œâ”€ DOCX â†’ python-docx
    â”œâ”€ JSON â†’ json parser
    â””â”€ TXT/MD â†’ text reader
    â†“
Split into chunks (500 chars)
    â†“
Generate embeddings (Sentence Transformers)
    â†“
Store in ChromaDB (Vector Database)
    â†“
âœ… Ready for search
```

**2. Query Flow (RAG):**
```
User asks question
    â†“
RAG System (à¹€à¸¥à¸·à¸­à¸ 1 à¸ˆà¸²à¸ 6 à¹à¸šà¸š)
    â”œâ”€ Naive: Vector search
    â”œâ”€ Contextual: Context-aware
    â”œâ”€ Rerank: Vector + Reranking
    â”œâ”€ Hybrid: BM25 + Vector ğŸ”¥
    â”œâ”€ Query Rewrite: Multiple queries
    â””â”€ Multi-step: Iterative search
    â†“
Retrieve relevant chunks (k=3)
    â†“
Extract images (if any)
    â†“
Build context + prompt
    â†“
Send to LLM (Llama/Qwen/DeepSeek)
    â†“
Generate answer
    â†“
Display to user + images
```

**3. Model Switching Flow:**
```
User selects new model
    â†“
Clear cache
    â†“
Initialize new LLMHandler
    â†“
Reinitialize RAG system
    â†“
âœ… Ready with new model
    (Documents remain unchanged)
```

---

## ğŸ“Š Performance

### Chat Response Time

| Model | RAM Usage | Speed | Quality |
|-------|-----------|-------|---------|
| Llama 3.2:1b | ~2GB | âš¡âš¡âš¡ | â­â­ |
| Llama 3.2:3b | ~4GB | âš¡âš¡âš¡ | â­â­â­ |
| Qwen 2.5:7b | ~8GB | âš¡âš¡ | â­â­â­â­ |
| Llama 3.1:8b | ~8GB | âš¡âš¡ | â­â­â­ |
| DeepSeek R1:8b | ~8GB | âš¡âš¡ | â­â­â­â­ |
| Qwen 2.5:14b | ~16GB | âš¡ | â­â­â­â­â­ |

### Upload Performance

| Task | Time |
|------|------|
| Upload 1 file (500 KB) | 5-10s |
| Upload 5 files (2.5 MB) | 15-30s |
| Embedding generation | ~0.02s/chunk |
| Semantic Search | <0.5s |

*Tested on: RTX 4070 Ti 12GB, 64GB RAM*

### RAG Comparison

| RAG Type | Speed | Accuracy | Best For |
|----------|-------|----------|----------|
| Naive | âš¡âš¡âš¡ | â­â­ | à¸—à¸±à¹ˆà¸§à¹„à¸› |
| Contextual | âš¡âš¡âš¡ | â­â­â­ | à¸—à¸±à¹ˆà¸§à¹„à¸› |
| Rerank | âš¡âš¡ | â­â­â­ | à¸„à¸§à¸²à¸¡à¹à¸¡à¹ˆà¸™à¸¢à¸³ |
| Hybrid ğŸ”¥ | âš¡âš¡ | â­â­â­â­ | à¸ à¸²à¸©à¸²à¹„à¸—à¸¢ |
| Query Rewrite | âš¡âš¡ | â­â­â­ | à¸„à¸³à¸–à¸²à¸¡à¸„à¸¥à¸¸à¸¡à¹€à¸„à¸£à¸·à¸­ |
| Multi-step | âš¡ | â­â­â­â­ | à¸„à¸³à¸–à¸²à¸¡à¸‹à¸±à¸šà¸‹à¹‰à¸­à¸™ |

---

## ğŸ“ Raspberry Pi Support

à¸£à¸°à¸šà¸šà¸£à¸­à¸‡à¸£à¸±à¸š Raspberry Pi! ğŸ‰

### Recommended Models

| Raspberry Pi | RAM | Model | Performance |
|-------------|-----|-------|-------------|
| **Pi 4 (2GB)** | 2GB | llama3.2:1b | ğŸŒ à¸Šà¹‰à¸² |
| **Pi 4 (4GB)** | 4GB | llama3.2:3b | ğŸ¢ à¸›à¸²à¸™à¸à¸¥à¸²à¸‡ |
| **Pi 5 (8GB)** | 8GB | qwen2.5:3b | ğŸš¶ à¹ƒà¸Šà¹‰à¹„à¸”à¹‰ |

### Installation on Pi

```bash
# 1. Update system
sudo apt update && sudo apt upgrade -y

# 2. Install dependencies
sudo apt install -y python3-pip python3-venv git

# 3. Increase swap (important!)
sudo dphys-swapfile swapoff
sudo nano /etc/dphys-swapfile
# Set: CONF_SWAPSIZE=4096
sudo dphys-swapfile setup
sudo dphys-swapfile swapon

# 4. Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# 5. Clone and install
git clone https://github.com/6amdev/personal-ai-assistant.git
cd personal-ai-assistant
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# 6. Pull lightweight model
ollama pull llama3.2:3b

# 7. Run
streamlit run app.py --server.address 0.0.0.0
```

Access from another device: `http://[Pi-IP]:8501`

---

## ğŸ› Troubleshooting

### Ollama not found
```bash
# Check if Ollama is running
ollama --version

# If not found, install
curl -fsSL https://ollama.com/install.sh | sh
```

### Model not found
```bash
# List available models
ollama list

# Pull missing model
ollama pull llama3.1:8b
```

### Import error
```bash
# Reinstall dependencies
pip install --upgrade -r requirements.txt
```

### Out of Memory
```bash
# Use smaller model
ollama pull llama3.2:3b

# Or increase swap (Linux)
sudo fallocate -l 4G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
```

### Slow performance
**Solutions:**
- à¹ƒà¸Šà¹‰ GPU (à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡ CUDA drivers)
- à¹ƒà¸Šà¹‰ model à¹€à¸¥à¹‡à¸à¸à¸§à¹ˆà¸² (llama3.2:3b)
- à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™ embedding device à¹€à¸›à¹‡à¸™ `cpu` à¹ƒà¸™ `.env`
- à¸¥à¸” `CHUNK_SIZE` à¹ƒà¸™ `.env`

### CUDA not available
```bash
# Edit .env
EMBEDDING_DEVICE=cpu  # Change from cuda to cpu
```

---

## ğŸ“ Project Structure

```
personal-ai-assistant/
â”œâ”€â”€ app.py                      # Main application
â”œâ”€â”€ config.py                   # Configuration
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ .env                        # Environment variables
â”œâ”€â”€ .env.example               # Example config
â”œâ”€â”€ README.md                  # This file
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ llm.py                 # LLM Handler (Multi-model)
â”‚   â”œâ”€â”€ memory.py              # Memory & Vector DB
â”‚   â”œâ”€â”€ ui.py                  # Streamlit UI (Model selector)
â”‚   â”œâ”€â”€ document_processor.py  # Document processing
â”‚   â”œâ”€â”€ utils.py               # Image handler
â”‚   â”‚
â”‚   â””â”€â”€ rag/                   # RAG implementations
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ base_rag.py        # Base class
â”‚       â”œâ”€â”€ naive_rag.py       # Naive RAG
â”‚       â”œâ”€â”€ contextual_rag.py  # Contextual RAG
â”‚       â”œâ”€â”€ rerank_rag.py      # Rerank RAG
â”‚       â”œâ”€â”€ hybrid_rag.py      # Hybrid RAG ğŸ”¥
â”‚       â”œâ”€â”€ query_rewrite_rag.py
â”‚       â””â”€â”€ multistep_rag.py
â”‚
â””â”€â”€ data/
    â”œâ”€â”€ chroma_db/             # Vector database (auto-created)
    â””â”€â”€ images/                # Uploaded images (optional)
```

---

## ğŸ¯ Configuration

Edit `.env` file:

```bash
# LLM Settings
LLM_MODEL=llama3.1:8b           # Default model
LLM_TEMPERATURE=0.7             # Creativity (0-1)

# Embeddings
EMBEDDING_PROVIDER=sentence-transformers
SENTENCE_TRANSFORMER_MODEL=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
EMBEDDING_DEVICE=cuda           # cuda or cpu

# Database
CHROMA_DB_DIR=./data/chroma_db
COLLECTION_NAME=personal_assistant

# Document Processing
CHUNK_SIZE=500                  # Chunk size in characters
CHUNK_OVERLAP=50                # Overlap between chunks
```

---

## ğŸ¨ Advanced Features

### Image Support
- à¸£à¸°à¸šà¸šà¸”à¸¶à¸‡à¸£à¸¹à¸›à¸ à¸²à¸à¸ˆà¸²à¸ JSON/URL à¹‚à¸”à¸¢à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´
- à¹à¸ªà¸”à¸‡à¸”à¹‰à¸§à¸¢ Lightbox (à¸„à¸¥à¸´à¸à¸‚à¸¢à¸²à¸¢à¹„à¸”à¹‰)
- à¸£à¸­à¸‡à¸£à¸±à¸š URL, Local file, Base64

### Debug Mode
- à¹€à¸›à¸´à¸” Debug Mode à¹ƒà¸™ sidebar
- à¸”à¸¹à¸‚à¹‰à¸­à¸¡à¸¹à¸¥: RAG type, Context length, Sources, Images
- à¹€à¸«à¸¡à¸²à¸°à¸à¸±à¸šà¸à¸²à¸£à¸—à¸”à¸ªà¸­à¸šà¹à¸¥à¸° fine-tune

### Document Management
- à¸”à¸¹à¸£à¸²à¸¢à¸à¸²à¸£à¹€à¸­à¸à¸ªà¸²à¸£à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”
- à¸¥à¸šà¸—à¸µà¸¥à¸°à¹„à¸Ÿà¸¥à¹Œ
- à¸¥à¸šà¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¹„à¸”à¹‰

---

## ğŸ¤ Contributing

Fork â†’ Branch â†’ Commit â†’ Push â†’ Pull Request

### Development Setup

```bash
# Clone your fork
git clone https://github.com/YOUR_USERNAME/personal-ai-assistant.git

# Create branch
git checkout -b feature/amazing-feature

# Make changes and test
streamlit run app.py

# Commit
git commit -m "Add amazing feature"

# Push
git push origin feature/amazing-feature

# Create Pull Request on GitHub
```

---

## ğŸ“ License

MIT License - à¹ƒà¸Šà¹‰à¸Ÿà¸£à¸µ à¸›à¸£à¸±à¸šà¹à¸•à¹ˆà¸‡à¹„à¸”à¹‰ à¹à¸Šà¸£à¹Œà¸•à¹ˆà¸­à¹„à¸”à¹‰

---

## ğŸ™ Acknowledgments

Special thanks to:
- **Meta AI** for Llama 3.1/3.2
- **Alibaba** for Qwen 2.5
- **DeepSeek** for DeepSeek R1
- **Ollama team** for local LLM runtime
- **LangChain** for the amazing framework
- **Streamlit** for the beautiful UI
- **ChromaDB** for vector storage
- **Sentence Transformers** for fast embeddings

---

## ğŸ“§ Contact & Support

**6amdev** 
- ğŸ™ GitHub: [github.com/6amdev](https://github.com/6amdev)
- ğŸ› Issues: [Report bugs](https://github.com/6amdev/personal-ai-assistant/issues)
- ğŸ’¬ Discussions: [Ask questions](https://github.com/6amdev/personal-ai-assistant/discussions)

---

## ğŸŒŸ Star History

If you find this project useful, please give it a star! â­

---

**Made with â¤ï¸ by [6amdev](https://github.com/6amdev)**

*Your data, your AI, your rules.* ğŸ”’