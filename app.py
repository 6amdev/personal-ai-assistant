"""Personal AI Assistant - Main App (Final Fix)"""
import streamlit as st
import tempfile
import os
from pathlib import Path

from src.ui import setup_page, show_header, show_sidebar, chat_interface, clear_chat_history
from src.llm import LLMHandler
from src.memory import MemoryHandler
from src.document_processor import DocumentProcessor
from src.rag import (
    NaiveRAG, 
    ContextualRAG, 
    RerankRAG, 
    HybridRAG, 
    QueryRewriteRAG, 
    MultiStepRAG
)

def init_session_state():
    """Initialize all session state variables ONCE"""
    if 'app_initialized' not in st.session_state:
        from config import LLM_MODEL
        import torch
        
        st.session_state.selected_model = LLM_MODEL
        st.session_state.processing_device = "GPU" if torch.cuda.is_available() else "CPU"
        st.session_state.embedding_device = "cuda" if torch.cuda.is_available() else "cpu"
        st.session_state.app_initialized = True
        st.session_state.components_loaded = False
        
        print("üîß Session state initialized")


def load_components():
    """Load LLM and Memory components"""
    if st.session_state.components_loaded:
        return st.session_state.llm, st.session_state.memory
    
    selected_model = st.session_state.selected_model
    llm_device = st.session_state.processing_device
    embedding_device = st.session_state.embedding_device
    
    print(f"üîÑ Initializing...")
    print(f"   Model: {selected_model}")
    print(f"   LLM Device: {llm_device}")
    print(f"   Embedding Device: {embedding_device}")
    
    try:
        llm = LLMHandler(model_name=selected_model, device=llm_device)
        memory = MemoryHandler(device=embedding_device)
        
        st.session_state.llm = llm
        st.session_state.memory = memory
        st.session_state.components_loaded = True
        
        print("‚úÖ Components initialized!")
        return llm, memory
        
    except Exception as e:
        st.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}")
        st.info("üí° ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Ollama ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡∏π‡πà!")
        st.code("ollama list", language="bash")
        st.stop()


def main():
    setup_page()
    
    # Initialize session state
    init_session_state()
    
    # Load components
    with st.spinner("üîÑ Loading AI components..."):
        llm, memory = load_components()
    
    show_header()
    
    # üî• Sidebar - ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤ ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£
    sidebar_returns = show_sidebar(memory)
    
    # Unpack returns
    if len(sidebar_returns) >= 6:
        if len(sidebar_returns) == 9:
            clear_chat, clear_memory, uploaded_files, docs_to_delete, debug_mode, rag_type, new_model, new_llm_dev, new_emb_dev = sidebar_returns
        elif len(sidebar_returns) == 6:
            clear_chat, clear_memory, uploaded_files, docs_to_delete, debug_mode, rag_type = sidebar_returns
            new_model = st.session_state.selected_model
            new_llm_dev = st.session_state.processing_device
            new_emb_dev = st.session_state.embedding_device
        else:
            st.error(f"‚ö†Ô∏è Unexpected sidebar returns: {len(sidebar_returns)}")
            st.stop()
    else:
        st.error(f"‚ö†Ô∏è Sidebar return {len(sidebar_returns)} values")
        st.stop()
    
    st.session_state.debug_mode = debug_mode
    
    # üî• ‡πÄ‡∏ä‡πá‡∏Ñ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á settings (‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà rerun ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ)
    settings_changed = False
    change_message = []
    
    if new_model != st.session_state.selected_model:
        st.session_state.selected_model = new_model
        st.session_state.components_loaded = False
        settings_changed = True
        change_message.append(f"Model: {new_model}")
    
    if new_llm_dev != st.session_state.processing_device:
        st.session_state.processing_device = new_llm_dev
        st.session_state.components_loaded = False
        settings_changed = True
        change_message.append(f"LLM Device: {new_llm_dev}")
    
    if new_emb_dev != st.session_state.embedding_device:
        st.session_state.embedding_device = new_emb_dev
        st.session_state.components_loaded = False
        settings_changed = True
        change_message.append(f"Embedding Device: {new_emb_dev}")
    
    # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡∏£‡∏≠ user action
    if settings_changed:
        st.sidebar.warning("‚ö†Ô∏è Settings changed:")
        for msg in change_message:
            st.sidebar.info(f"‚Ä¢ {msg}")
        
        if st.sidebar.button("üîÑ Apply Changes", type="primary"):
            st.rerun()
        else:
            st.sidebar.info("üëÜ Click to apply")
            # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°‡∏ï‡πà‡∏≠‡πÑ‡∏õ
            llm, memory = load_components()
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á RAG system
    if "current_rag_type" not in st.session_state or st.session_state.current_rag_type != rag_type:
        st.session_state.current_rag_type = rag_type
        
        if rag_type == "Naive RAG":
            st.session_state.rag_system = NaiveRAG(llm, memory)
        elif rag_type == "Contextual RAG":
            st.session_state.rag_system = ContextualRAG(llm, memory)
        elif rag_type == "Rerank RAG":
            st.session_state.rag_system = RerankRAG(llm, memory)
        elif rag_type == "Hybrid RAG":
            st.session_state.rag_system = HybridRAG(llm, memory)
        elif rag_type == "Query Rewrite RAG":
            st.session_state.rag_system = QueryRewriteRAG(llm, memory)
        elif rag_type == "Multi-step RAG":
            st.session_state.rag_system = MultiStepRAG(llm, memory)
    
    rag = st.session_state.rag_system
    
    # Handle clear chat
    if clear_chat:
        clear_chat_history()
        memory.clear_conversation()
    
    # Handle delete specific documents
    if docs_to_delete:
        for doc_source in docs_to_delete:
            deleted_count = memory.delete_by_source(doc_source)
            if deleted_count > 0:
                st.success(f"‚úÖ ‡∏•‡∏ö {doc_source} ‡πÅ‡∏•‡πâ‡∏ß ({deleted_count} chunks)")
                st.rerun()
    
    # Handle clear all documents
    if clear_memory:
        with st.spinner("üóëÔ∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏•‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î..."):
            success = memory.clear_all_documents()
            
            if success:
                st.success("‚úÖ ‡∏•‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÅ‡∏•‡πâ‡∏ß!")
                st.rerun()
            else:
                st.error("‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£")
    
    # Handle file uploads
    if uploaded_files:
        process_uploaded_files(uploaded_files, memory)
    
    # Main chat
    chat_interface(llm, memory, rag_system=rag)
    
    # Footer
    st.markdown("---")
    st.markdown(f"""
    <div style='text-align: center; color: #666;'>
        Made with ‚ù§Ô∏è using <strong>{st.session_state.selected_model}</strong> ({st.session_state.processing_device}) ‚Ä¢ Embeddings ({st.session_state.embedding_device.upper()})
    </div>
    """, unsafe_allow_html=True)


def process_uploaded_files(uploaded_files, memory):
    """Process and store uploaded files with detailed progress"""
    
    # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ã‡πâ‡∏≥
    file_ids = [f.file_id for f in uploaded_files]
    processed_key = "processed_files"
    
    if processed_key not in st.session_state:
        st.session_state[processed_key] = set()
    
    # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á
    new_files = []
    for f in uploaded_files:
        if f.file_id not in st.session_state[processed_key]:
            new_files.append(f)
            st.session_state[processed_key].add(f.file_id)
    
    if not new_files:
        return  # ‡πÑ‡∏ü‡∏•‡πå‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    detail_text = st.empty()
    
    processor = DocumentProcessor()
    total_files = len(new_files)
    all_chunks = []
    all_metadatas = []
    
    # Phase 1: ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    for idx, uploaded_file in enumerate(new_files):
        try:
            status_text.text(f"üìÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô {uploaded_file.name}... ({idx+1}/{total_files})")
            
            # Save temp file
            with tempfile.NamedTemporaryFile(delete=False, suffix=Path(uploaded_file.name).suffix) as tmp_file:
                tmp_file.write(uploaded_file.getvalue())
                tmp_path = tmp_file.name
            
            # Process
            status_text.text(f"üìù ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å {uploaded_file.name}...")
            chunks = processor.process_file(tmp_path)
            
            # ‡∏£‡∏ß‡∏° chunks
            all_chunks.extend(chunks)
            all_metadatas.extend([{'source': uploaded_file.name} for _ in chunks])
            
            detail_text.info(f"‚úÖ {uploaded_file.name}: {len(chunks)} chunks")
            
            # Cleanup
            os.unlink(tmp_path)
            
            # Update progress
            progress_bar.progress((idx + 1) / total_files * 0.4)
            
        except Exception as e:
            st.error(f"‚ùå {uploaded_file.name}: {e}")
    
    # Phase 2: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡∏µ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
    if all_chunks:
        total_chunks = len(all_chunks)
        status_text.text(f"üß† ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å {total_chunks} chunks...")
        
        try:
            import time
            start_time = time.time()
            
            progress_bar.progress(0.5)
            
            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß!
            memory.add_documents(all_chunks, all_metadatas)
            
            elapsed = time.time() - start_time
            
            progress_bar.progress(1.0)
            status_text.empty()
            detail_text.empty()
            
            st.success(f"‚úÖ ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! ({total_chunks} chunks ‡∏à‡∏≤‡∏Å {total_files} ‡πÑ‡∏ü‡∏•‡πå)")
            st.info(f"‚è±Ô∏è ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤: {elapsed:.1f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ (~{elapsed/total_chunks:.2f}s/chunk)")
            st.balloons()
            
            st.rerun()
            
        except Exception as e:
            progress_bar.empty()
            status_text.empty()
            detail_text.empty()
            st.error(f"‚ùå Error: {e}")


if __name__ == "__main__":
    main()