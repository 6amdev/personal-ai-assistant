# ü§ñ ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠ AI ‡πÅ‡∏•‡∏∞ Machine Learning ‡∏â‡∏ö‡∏±‡∏ö‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå

> ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ AI, Machine Learning, ‡πÅ‡∏•‡∏∞ Deep Learning

---

## üìö ‡∏™‡∏≤‡∏£‡∏ö‡∏±‡∏ç

1. [AI ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?](#ai-‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£)
2. [Machine Learning ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?](#machine-learning-‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£)
3. [‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏≠‡∏á Machine Learning](#‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏≠‡∏á-machine-learning)
4. [Deep Learning](#deep-learning)
5. [Neural Networks](#neural-networks)
6. [‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô](#‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô)
7. [‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡πÅ‡∏•‡∏∞ Libraries](#‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡πÅ‡∏•‡∏∞-libraries)
8. [‡πÅ‡∏´‡∏•‡πà‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ](#‡πÅ‡∏´‡∏•‡πà‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ)

---

## AI ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?

**Artificial Intelligence (AI)** ‡∏´‡∏£‡∏∑‡∏≠ ‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏õ‡∏£‡∏∞‡∏î‡∏¥‡∏©‡∏ê‡πå ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏™‡∏ï‡∏¥‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏Ç‡∏≠‡∏á‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå‡πÑ‡∏î‡πâ

### ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤

- **1950** - Alan Turing ‡πÄ‡∏™‡∏ô‡∏≠ Turing Test
- **1956** - John McCarthy ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "Artificial Intelligence" ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å
- **1997** - Deep Blue ‡∏ä‡∏ô‡∏∞ Garry Kasparov ‡πÉ‡∏ô‡∏´‡∏°‡∏≤‡∏Å‡∏£‡∏∏‡∏Å
- **2011** - IBM Watson ‡∏ä‡∏ô‡∏∞ Jeopardy
- **2016** - AlphaGo ‡∏ä‡∏ô‡∏∞ Lee Sedol ‡πÉ‡∏ô‡∏´‡∏°‡∏≤‡∏Å‡∏•‡πâ‡∏≠‡∏°
- **2022** - ChatGPT ‡πÄ‡∏õ‡∏¥‡∏î‡∏ï‡∏±‡∏ß ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡∏Å‡∏≤‡∏£‡∏ì‡πå‡πÇ‡∏•‡∏Å

### ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏≠‡∏á AI

#### 1. Narrow AI (Weak AI)
- AI ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏î‡πâ‡∏≤‡∏ô
- ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: Siri, Alexa, ‡∏Å‡∏≤‡∏£‡∏à‡∏î‡∏à‡∏≥‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
- **‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô**

#### 2. General AI (Strong AI)
- AI ‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ñ‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå
- ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏à‡∏£‡∏¥‡∏á ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ó‡∏§‡∏©‡∏é‡∏µ
- ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï

#### 3. Super AI
- AI ‡∏ó‡∏µ‡πà‡∏â‡∏•‡∏≤‡∏î‡∏Å‡∏ß‡πà‡∏≤‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå‡∏ó‡∏∏‡∏Å‡∏î‡πâ‡∏≤‡∏ô
- ‡∏¢‡∏±‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏¥‡∏ô‡∏ï‡∏ô‡∏≤‡∏Å‡∏≤‡∏£
- ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ñ‡πà‡∏≤‡∏¢‡∏ó‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏ô‡∏±‡∏á‡∏°‡∏≤‡∏Å‡∏°‡∏≤‡∏¢

---

## Machine Learning ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?

**Machine Learning (ML)** ‡∏Ñ‡∏∑‡∏≠‡∏™‡∏≤‡∏Ç‡∏≤‡∏¢‡πà‡∏≠‡∏¢‡∏Ç‡∏≠‡∏á AI ‡∏ó‡∏µ‡πà‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡πÅ‡∏ö‡∏ö‡∏ï‡∏≤‡∏¢‡∏ï‡∏±‡∏ß

### ‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô

```
‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Data) ‚Üí Algorithm ‚Üí ‡πÇ‡∏°‡πÄ‡∏î‡∏• (Model) ‚Üí ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ (Prediction)
```

### ‡∏™‡∏π‡∏ï‡∏£‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô

**Linear Regression:**
```
y = mx + b
```
- y = ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
- x = ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ (input)
- m = ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏ô (slope)
- b = ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏á‡∏ó‡∏µ‡πà (intercept)

**Cost Function (Mean Squared Error):**
```
MSE = (1/n) Œ£(y_actual - y_predicted)¬≤
```

---

## ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏≠‡∏á Machine Learning

### 1. Supervised Learning (‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÅ‡∏ö‡∏ö‡∏°‡∏µ‡∏ú‡∏π‡πâ‡∏™‡∏≠‡∏ô)

**‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£:** ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö (labeled data) ‡πÅ‡∏Å‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:**
- **Classification** - ‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó
  - ‡∏™‡πÅ‡∏õ‡∏°‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏™‡πÅ‡∏õ‡∏°
  - ‡πÅ‡∏°‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡∏´‡∏°‡∏≤
  - ‡πÇ‡∏£‡∏Ñ‡∏°‡∏∞‡πÄ‡∏£‡πá‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà

- **Regression** - ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á
  - ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ö‡πâ‡∏≤‡∏ô
  - ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢
  - ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥

**Algorithms ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ:**
- Linear Regression
- Logistic Regression
- Decision Tree
- Random Forest
- Support Vector Machine (SVM)
- K-Nearest Neighbors (KNN)
- Neural Networks

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î Python:**
```python
from sklearn.linear_model import LinearRegression

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•
model = LinearRegression()

# ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
model.fit(X_train, y_train)

# ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
predictions = model.predict(X_test)
```

---

### 2. Unsupervised Learning (‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ú‡∏π‡πâ‡∏™‡∏≠‡∏ô)

**‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£:** ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö (unlabeled data) ‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏≤‡πÅ‡∏û‡∏ó‡πÄ‡∏ó‡∏¥‡∏£‡πå‡∏ô‡πÄ‡∏≠‡∏á

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:**
- **Clustering** - ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°
  - ‡πÅ‡∏ö‡πà‡∏á‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤ (Customer Segmentation)
  - ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏Ç‡πà‡∏≤‡∏ß
  - ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏¢‡∏µ‡∏ô

- **Dimensionality Reduction** - ‡∏•‡∏î‡∏°‡∏¥‡∏ï‡∏¥‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
  - Principal Component Analysis (PCA)
  - t-SNE
  - UMAP

- **Association Rules** - ‡∏´‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå
  - Market Basket Analysis
  - "‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏ã‡∏∑‡πâ‡∏≠‡∏Ç‡∏ô‡∏°‡∏õ‡∏±‡∏á‡∏°‡∏±‡∏Å‡∏ã‡∏∑‡πâ‡∏≠‡∏ô‡∏°‡∏î‡πâ‡∏ß‡∏¢"

**Algorithms ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ:**
- K-Means Clustering
- Hierarchical Clustering
- DBSCAN
- Gaussian Mixture Models
- PCA
- Autoencoders

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î:**
```python
from sklearn.cluster import KMeans

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• K-Means (3 ‡∏Å‡∏•‡∏∏‡πà‡∏°)
kmeans = KMeans(n_clusters=3)

# ‡∏ù‡∏∂‡∏Å‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
clusters = kmeans.fit_predict(X)
```

---

### 3. Reinforcement Learning (‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÅ‡∏ö‡∏ö‡πÄ‡∏™‡∏£‡∏¥‡∏°‡πÅ‡∏£‡∏á)

**‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£:** ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏•‡∏≠‡∏á‡∏ú‡∏¥‡∏î‡∏•‡∏≠‡∏á‡∏ñ‡∏π‡∏Å ‡πÇ‡∏î‡∏¢‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö reward ‡∏´‡∏£‡∏∑‡∏≠ penalty

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:**
- ‡πÄ‡∏Å‡∏° (AlphaGo, Dota 2, Chess)
- ‡∏´‡∏∏‡πà‡∏ô‡∏¢‡∏ô‡∏ï‡πå‡πÄ‡∏î‡∏¥‡∏ô
- ‡∏£‡∏ñ‡∏¢‡∏ô‡∏ï‡πå‡∏Ç‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
- ‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ (Recommendation System)

**‡∏≠‡∏á‡∏Ñ‡πå‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏´‡∏•‡∏±‡∏Å:**
- **Agent** - ‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ
- **Environment** - ‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°
- **State** - ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
- **Action** - ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏ó‡∏≥
- **Reward** - ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•/‡πÇ‡∏ó‡∏©

**Algorithms ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ:**
- Q-Learning
- Deep Q-Network (DQN)
- Policy Gradient
- Actor-Critic
- Proximal Policy Optimization (PPO)

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î (Q-Learning):**
```python
import numpy as np

# Q-Table
Q = np.zeros([state_space, action_space])

# Q-Learning Algorithm
for episode in range(num_episodes):
    state = env.reset()
    
    for step in range(max_steps):
        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å action
        action = np.argmax(Q[state, :])
        
        # ‡∏ó‡∏≥ action
        new_state, reward, done = env.step(action)
        
        # Update Q-Table
        Q[state, action] = Q[state, action] + \
            learning_rate * (reward + discount_factor * 
            np.max(Q[new_state, :]) - Q[state, action])
        
        state = new_state
        
        if done:
            break
```

---

## Deep Learning

**Deep Learning** ‡∏Ñ‡∏∑‡∏≠‡∏™‡∏≤‡∏Ç‡∏≤‡∏¢‡πà‡∏≠‡∏¢‡∏Ç‡∏≠‡∏á Machine Learning ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ Neural Networks ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢‡∏ä‡∏±‡πâ‡∏ô (deep)

### ‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á Deep Learning?

**‡∏Ç‡πâ‡∏≠‡∏î‡∏µ:**
- ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ (‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û, ‡πÄ‡∏™‡∏µ‡∏¢‡∏á, ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°)
- ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ feature ‡πÄ‡∏≠‡∏á (automatic feature extraction)
- ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏¢‡∏≠‡∏∞
- State-of-the-art ‡πÉ‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏á‡∏≤‡∏ô

**‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢:**
- ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏¢‡∏≠‡∏∞‡∏°‡∏≤‡∏Å
- ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ GPU (‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ô‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏´‡∏ô‡∏±‡∏Å)
- ‡∏¢‡∏≤‡∏Å‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£ interpret (black box)
- ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏≤‡∏á

### ‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏°‡∏ó‡∏µ‡πà‡∏ô‡∏¥‡∏¢‡∏°

#### 1. Convolutional Neural Networks (CNN)
**‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:** ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û, ‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠

**‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠:**
- LeNet (1998)
- AlexNet (2012) - ‡∏ä‡∏ô‡∏∞ ImageNet
- VGGNet (2014)
- ResNet (2015)
- EfficientNet (2019)

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î (Keras):**
```python
from tensorflow.keras import layers, models

model = models.Sequential([
    # Convolution Layer
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    layers.MaxPooling2D((2, 2)),
    
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    
    layers.Conv2D(64, (3, 3), activation='relu'),
    
    # Fully Connected Layer
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
```

---

#### 2. Recurrent Neural Networks (RNN)
**‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:** ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö sequence (‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°, ‡πÄ‡∏™‡∏µ‡∏¢‡∏á, time series)

**‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó:**
- Vanilla RNN
- Long Short-Term Memory (LSTM) - ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤ RNN
- Gated Recurrent Unit (GRU) - ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ LSTM

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î (LSTM):**
```python
from tensorflow.keras import layers, models

model = models.Sequential([
    layers.Embedding(vocab_size, 128),
    layers.LSTM(128, return_sequences=True),
    layers.LSTM(64),
    layers.Dense(64, activation='relu'),
    layers.Dense(num_classes, activation='softmax')
])
```

---

#### 3. Transformer
**‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:** NLP (Natural Language Processing)

**‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠:**
- BERT (Google, 2018)
- GPT-2, GPT-3, GPT-4 (OpenAI)
- T5 (Google)
- LLaMA (Meta)
- Claude (Anthropic)

**‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏°‡∏´‡∏•‡∏±‡∏Å:**
- Self-Attention Mechanism
- Multi-Head Attention
- Positional Encoding
- Feed-Forward Networks

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ (Hugging Face):**
```python
from transformers import pipeline

# Text Generation
generator = pipeline('text-generation', model='gpt2')
result = generator("Once upon a time", max_length=50)

# Sentiment Analysis
classifier = pipeline('sentiment-analysis')
result = classifier("I love this product!")

# Question Answering
qa = pipeline('question-answering')
result = qa(question="What is AI?", 
            context="AI is artificial intelligence...")
```

---

#### 4. Generative Adversarial Networks (GANs)
**‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:** ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà (‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û, ‡πÄ‡∏™‡∏µ‡∏¢‡∏á, ‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠)

**‡∏≠‡∏á‡∏Ñ‡πå‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö:**
- **Generator** - ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏•‡∏≠‡∏°
- **Discriminator** - ‡πÅ‡∏¢‡∏Å‡∏Ç‡∏≠‡∏á‡∏à‡∏£‡∏¥‡∏á/‡∏õ‡∏•‡∏≠‡∏°
- **‡πÅ‡∏Ç‡πà‡∏á‡∏Å‡∏±‡∏ô** - Generator ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏´‡∏•‡∏≠‡∏Å, Discriminator ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏à‡∏±‡∏ö

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ:**
- ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏π‡∏õ‡∏´‡∏ô‡πâ‡∏≤‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏à‡∏£‡∏¥‡∏á (ThisPersonDoesNotExist.com)
- ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏á‡∏≤‡∏ô‡∏®‡∏¥‡∏•‡∏õ‡∏∞
- ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û (Super Resolution)
- ‡πÅ‡∏õ‡∏•‡∏á‡∏£‡∏π‡∏õ‡∏£‡πà‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏Ñ‡∏ô (Face Swap)

---

#### 5. Diffusion Models
**‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:** ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á

**‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠:**
- DALL-E 2 (OpenAI)
- Stable Diffusion (Stability AI)
- Midjourney
- Imagen (Google)

**‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô:**
1. ‡πÄ‡∏û‡∏¥‡πà‡∏° noise ‡πÉ‡∏´‡πâ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡∏•‡∏∞‡∏ô‡∏¥‡∏î
2. ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡πâ‡πÄ‡∏≠‡∏≤ noise ‡∏≠‡∏≠‡∏Å
3. ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å noise ‚Üí ‡∏Ñ‡πà‡∏≠‡∏¢‡πÜ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û

---

## Neural Networks

### ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô

```
Input Layer ‚Üí Hidden Layers ‚Üí Output Layer
```

**‡∏™‡πà‡∏ß‡∏ô‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö:**
- **Neurons (Nodes)** - ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì
- **Weights** - ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç)
- **Bias** - ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏á‡∏ó‡∏µ‡πà
- **Activation Function** - ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏Å‡∏£‡∏∞‡∏ï‡∏∏‡πâ‡∏ô

### Activation Functions

#### 1. Sigmoid
```python
œÉ(x) = 1 / (1 + e^(-x))
```
- Output: 0 ‡∏ñ‡∏∂‡∏á 1
- ‡πÉ‡∏ä‡πâ: Binary Classification (‡∏ä‡∏±‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢)

#### 2. Tanh
```python
tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))
```
- Output: -1 ‡∏ñ‡∏∂‡∏á 1
- ‡πÉ‡∏ä‡πâ: Hidden Layers (‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤ Sigmoid)

#### 3. ReLU (Rectified Linear Unit)
```python
f(x) = max(0, x)
```
- Output: 0 ‡∏ñ‡∏∂‡∏á ‚àû
- ‡πÉ‡∏ä‡πâ: Hidden Layers (‡∏ô‡∏¥‡∏¢‡∏°‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î) ‚≠ê
- ‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏•‡∏∞‡∏î‡∏µ‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ

#### 4. Softmax
```python
œÉ(x_i) = e^(x_i) / Œ£(e^(x_j))
```
- Output: ‡∏ú‡∏•‡∏£‡∏ß‡∏° = 1 (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô)
- ‡πÉ‡∏ä‡πâ: Multi-class Classification (‡∏ä‡∏±‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢)

### Forward Propagation

```python
# Layer 1
z1 = W1 @ x + b1
a1 = relu(z1)

# Layer 2
z2 = W2 @ a1 + b2
a2 = relu(z2)

# Output Layer
z3 = W3 @ a2 + b3
output = softmax(z3)
```

### Backpropagation

**‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô:**
1. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Loss
2. ‡∏´‡∏≤ Gradient (Chain Rule)
3. Update Weights

```python
# Gradient Descent
W = W - learning_rate * dL/dW
b = b - learning_rate * dL/db
```

### Optimizers

#### 1. Stochastic Gradient Descent (SGD)
```python
W = W - learning_rate * gradient
```

#### 2. Adam (‡∏ô‡∏¥‡∏¢‡∏°‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î) ‚≠ê
```python
# ‡∏£‡∏ß‡∏° momentum + adaptive learning rate
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
```

#### 3. RMSprop
#### 4. AdaGrad
#### 5. AdaDelta

---

## ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô

### 1. Computer Vision (‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å‡∏†‡∏≤‡∏û)

**‡∏á‡∏≤‡∏ô:**
- Image Classification - ‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏†‡∏≤‡∏û
- Object Detection - ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏ (YOLO, Faster R-CNN)
- Semantic Segmentation - ‡πÅ‡∏¢‡∏Å‡∏™‡πà‡∏ß‡∏ô‡∏†‡∏≤‡∏û
- Face Recognition - ‡∏à‡∏î‡∏à‡∏≥‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
- Optical Character Recognition (OCR) - ‡∏≠‡πà‡∏≤‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ:**
- ‡∏£‡∏ñ‡∏¢‡∏ô‡∏ï‡πå‡∏Ç‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
- ‡∏£‡∏∞‡∏ö‡∏ö‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
- ‡πÅ‡∏≠‡∏û‡πÅ‡∏õ‡∏•‡∏†‡∏≤‡∏©‡∏≤‡∏à‡∏≤‡∏Å‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û (Google Lens)
- ‡∏ï‡∏£‡∏ß‡∏à‡∏´‡∏≤‡πÇ‡∏£‡∏Ñ‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û X-Ray

---

### 2. Natural Language Processing (‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏†‡∏≤‡∏©‡∏≤)

**‡∏á‡∏≤‡∏ô:**
- Text Classification - ‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
- Sentiment Analysis - ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å
- Named Entity Recognition (NER) - ‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏â‡∏û‡∏≤‡∏∞
- Machine Translation - ‡πÅ‡∏õ‡∏•‡∏†‡∏≤‡∏©‡∏≤
- Question Answering - ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
- Text Generation - ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
- Summarization - ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ:**
- ChatGPT, Claude, Gemini
- Google Translate
- Grammarly (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏†‡∏≤‡∏©‡∏≤)
- Email Spam Filter
- Virtual Assistants (Siri, Alexa)

---

### 3. Speech Recognition (‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å‡πÄ‡∏™‡∏µ‡∏¢‡∏á)

**‡∏á‡∏≤‡∏ô:**
- Speech to Text - ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
- Text to Speech - ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏™‡∏µ‡∏¢‡∏á
- Speaker Recognition - ‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î
- Emotion Detection - ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏à‡∏≤‡∏Å‡πÄ‡∏™‡∏µ‡∏¢‡∏á

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ:**
- Google Assistant
- Siri, Alexa
- Zoom Transcription
- Podcast Transcription

---

### 4. Recommendation Systems (‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)

**‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô:**
- Collaborative Filtering - ‡∏î‡∏π‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡∏Ñ‡∏ô‡∏≠‡∏∑‡πà‡∏ô
- Content-Based Filtering - ‡∏î‡∏π‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤
- Hybrid - ‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á 2 ‡πÅ‡∏ö‡∏ö

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ:**
- Netflix - ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏´‡∏ô‡∏±‡∏á
- YouTube - ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠
- Amazon - ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤
- Spotify - ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏û‡∏•‡∏á

---

### 5. Time Series Forecasting (‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï)

**‡∏á‡∏≤‡∏ô:**
- Stock Price Prediction - ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏£‡∏≤‡∏Ñ‡∏≤‡∏´‡∏∏‡πâ‡∏ô
- Weather Forecasting - ‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏≠‡∏≤‡∏Å‡∏≤‡∏®
- Sales Forecasting - ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢
- Energy Demand - ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏ü‡πâ‡∏≤

**Algorithms:**
- ARIMA
- LSTM
- Prophet (Facebook)
- Temporal Convolutional Networks (TCN)

---

### 6. Healthcare (‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û)

**‡∏á‡∏≤‡∏ô:**
- Disease Diagnosis - ‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢‡πÇ‡∏£‡∏Ñ
- Drug Discovery - ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏¢‡∏≤‡πÉ‡∏´‡∏°‡πà
- Medical Image Analysis - ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏†‡∏≤‡∏û‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå
- Patient Monitoring - ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:**
- ‡∏ï‡∏£‡∏ß‡∏à‡∏°‡∏∞‡πÄ‡∏£‡πá‡∏á‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û X-Ray
- ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÇ‡∏£‡∏Ñ‡∏´‡∏±‡∏ß‡πÉ‡∏à
- ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤

---

## ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡πÅ‡∏•‡∏∞ Libraries

### Python Libraries (‡∏ô‡∏¥‡∏¢‡∏°‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î)

#### 1. NumPy
**‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:** ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏°‡∏ó‡∏£‡∏¥‡∏Å‡∏ã‡πå, Array

```python
import numpy as np

# ‡∏™‡∏£‡πâ‡∏≤‡∏á array
arr = np.array([1, 2, 3, 4, 5])

# Matrix multiplication
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])
C = np.dot(A, B)
```

---

#### 2. Pandas
**‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:** ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (DataFrame)

```python
import pandas as pd

# ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
df = pd.read_csv('data.csv')

# ‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
print(df.head())
print(df.describe())

# Filter
filtered = df[df['age'] > 25]
```

---

#### 3. Matplotlib & Seaborn
**‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:** Data Visualization

```python
import matplotlib.pyplot as plt
import seaborn as sns

# Line plot
plt.plot(x, y)
plt.title('My Plot')
plt.xlabel('X axis')
plt.ylabel('Y axis')
plt.show()

# Heatmap
sns.heatmap(correlation_matrix, annot=True)
```

---

#### 4. Scikit-learn
**‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:** Machine Learning ‡πÅ‡∏ö‡∏ö‡∏î‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏¥‡∏°

```python
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
model = RandomForestClassifier()
model.fit(X_train, y_train)

# ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
predictions = model.predict(X_test)

# ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
accuracy = accuracy_score(y_test, predictions)
print(f'Accuracy: {accuracy:.2f}')
```

---

#### 5. TensorFlow & Keras
**‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:** Deep Learning (‡∏à‡∏≤‡∏Å Google)

```python
import tensorflow as tf
from tensorflow import keras

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•
model = keras.Sequential([
    keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(10, activation='softmax')
])

# Compile
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# ‡∏ù‡∏∂‡∏Å
model.fit(X_train, y_train, epochs=10, batch_size=32)

# ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
model.evaluate(X_test, y_test)
```

---

#### 6. PyTorch
**‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:** Deep Learning (‡∏à‡∏≤‡∏Å Meta)

```python
import torch
import torch.nn as nn

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•
class NeuralNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(28*28, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, 10)
        )
    
    def forward(self, x):
        x = self.flatten(x)
        logits = self.linear_relu_stack(x)
        return logits

# ‡∏™‡∏£‡πâ‡∏≤‡∏á instance
model = NeuralNetwork()
```

---

#### 7. Hugging Face Transformers
**‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:** NLP (Pre-trained Models)

```python
from transformers import pipeline

# Sentiment Analysis
classifier = pipeline("sentiment-analysis")
result = classifier("I love machine learning!")

# Text Generation
generator = pipeline("text-generation", model="gpt2")
result = generator("The future of AI is", max_length=30)

# Translation
translator = pipeline("translation_en_to_fr")
result = translator("Hello, how are you?")
```

---

### ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏≠‡∏∑‡πà‡∏ô‡πÜ

- **Jupyter Notebook** - ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡πÅ‡∏ö‡∏ö interactive
- **Google Colab** - Jupyter ‡∏ö‡∏ô Cloud (‡∏ü‡∏£‡∏µ GPU!)
- **Weights & Biases** - Track experiments
- **TensorBoard** - Visualize training
- **MLflow** - Manage ML lifecycle
- **Docker** - Deploy models

---

## ‡πÅ‡∏´‡∏•‡πà‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ

### ‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏≠‡∏≠‡∏ô‡πÑ‡∏•‡∏ô‡πå (‡∏ü‡∏£‡∏µ)

#### 1. Coursera
- **Machine Learning** by Andrew Ng ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
- **Deep Learning Specialization** by Andrew Ng
- **AI For Everyone** (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÇ‡∏Ñ‡πâ‡∏î)

#### 2. Fast.ai
- **Practical Deep Learning for Coders** (‡πÄ‡∏ô‡πâ‡∏ô‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥)
- ‡∏ü‡∏£‡∏µ 100%
- ‡πÉ‡∏ä‡πâ PyTorch

#### 3. YouTube Channels
- **3Blue1Brown** - Neural Networks ‡∏°‡∏µ‡∏†‡∏≤‡∏û‡∏™‡∏ß‡∏¢
- **StatQuest** - ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏á‡πà‡∏≤‡∏¢‡∏°‡∏≤‡∏Å
- **Sentdex** - Python & ML Tutorials
- **Two Minute Papers** - Paper ‡∏£‡∏µ‡∏ß‡∏¥‡∏ß
- **Yannic Kilcher** - Paper Deep Dive

#### 4. Kaggle
- **Kaggle Learn** - Courses ‡∏™‡∏±‡πâ‡∏ô‡πÜ
- **Competitions** - ‡∏•‡∏≠‡∏á‡πÅ‡∏Ç‡πà‡∏á
- **Notebooks** - ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ô‡∏≠‡∏∑‡πà‡∏ô

---

### ‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥

#### ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
1. **"Hands-On Machine Learning"** by Aur√©lien G√©ron ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
2. **"Python Machine Learning"** by Sebastian Raschka
3. **"Deep Learning with Python"** by Fran√ßois Chollet

#### ‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á
1. **"Deep Learning"** by Ian Goodfellow (‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏ô)
2. **"Pattern Recognition and Machine Learning"** by Bishop
3. **"Reinforcement Learning"** by Sutton & Barto

---

### Websites & Blogs

- **Papers with Code** - Paper + Code implementations
- **Towards Data Science** - Medium publication
- **Machine Learning Mastery** - Tutorials
- **Distill.pub** - Interactive papers
- **OpenAI Blog** - Research updates
- **Google AI Blog** - Google's research

---

### Podcasts

- **Lex Fridman Podcast** - ‡∏™‡∏±‡∏°‡∏†‡∏≤‡∏©‡∏ì‡πå‡∏ô‡∏±‡∏Å‡∏ß‡∏¥‡∏à‡∏±‡∏¢ AI
- **The TWIML AI Podcast** - ML & AI discussions
- **Data Skeptic** - Data Science & ML

---

## üéØ ‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ (Roadmap)

### ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô (3-6 ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô)

**‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1-2: ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô**
- Python Programming
- NumPy, Pandas
- Data Visualization (Matplotlib)
- Statistics & Probability

**‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ó‡∏µ‡πà 3-4: Machine Learning**
- Supervised Learning (Regression, Classification)
- Unsupervised Learning (Clustering)
- Scikit-learn
- ‡∏ó‡∏≥‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡∏á‡πà‡∏≤‡∏¢‡πÜ

**‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ó‡∏µ‡πà 5-6: ‡∏Ç‡∏±‡πâ‡∏ô‡∏Å‡∏•‡∏≤‡∏á**
- Feature Engineering
- Model Evaluation
- Cross-Validation
- Kaggle Competitions (‡∏á‡πà‡∏≤‡∏¢‡πÜ)

---

### ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Å‡∏•‡∏≤‡∏á (6-12 ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô)

**‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ó‡∏µ‡πà 7-9: Deep Learning**
- Neural Networks ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
- TensorFlow/Keras ‡∏´‡∏£‡∏∑‡∏≠ PyTorch
- CNN (Computer Vision)
- Transfer Learning

**‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ó‡∏µ‡πà 10-12: ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏≤‡∏á**
‡πÄ‡∏•‡∏∑‡∏≠‡∏Å 1 ‡∏î